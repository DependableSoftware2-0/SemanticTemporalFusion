{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5096fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger,CSVLogger\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import evidence_loss\n",
    "import uncertain_fusion\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count()/2)\n",
    "LOGGER_NAME = 'ZOOM_MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b2aef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69371133, 0.30628867])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.dirichlet([30,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02bf8a",
   "metadata": {},
   "source": [
    "## ZOOM MNIST Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc58da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Zoom_MNIST(MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(Zoom_MNIST, self).__init__(root,  train=train,\n",
    "                                         download=download,\n",
    "                                         transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "        self.zoom_images = 3\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_zoom_images(img):\n",
    "        '''\n",
    "        returns list of images \n",
    "        '''\n",
    "        original_size = 28\n",
    "        \n",
    "        #Randomly select 3 image sizes by using dirichlet stick breaking\n",
    "        image_sizes = (np.random.dirichlet([30,10,10])*28).astype(int)\n",
    "        #image_sizes = (np.random.dirichlet([30,10])*28).astype(int)\n",
    "        size = 0\n",
    "        x = int(np.random.rand() * 28)\n",
    "        y = int(np.random.rand() * 28)\n",
    "        \n",
    "        output_images = []\n",
    "\n",
    "        for i, image_size in enumerate(image_sizes):\n",
    "            size +=image_size\n",
    "            if (x + size)>original_size:\n",
    "                x  = x - (x + size - original_size)\n",
    "            if (y + size)>original_size:\n",
    "                y = y - (y + size - original_size)\n",
    "            canvas = np.zeros((original_size, original_size), dtype=np.uint8)\n",
    "            small_img = cv2.resize(img.numpy().reshape(original_size,original_size, 1), (size,size))\n",
    "            canvas[ y:y+small_img.shape[0], x:x+small_img.shape[1]] = small_img\n",
    "            output_images.append(canvas.reshape(original_size,original_size))\n",
    "        return output_images\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        images = self.create_zoom_images(img)\n",
    "        \n",
    "        out_images=[]\n",
    "        for img in images:\n",
    "            pil_img = Image.fromarray(img, mode='L')\n",
    "\n",
    "            if self.transform is not None:\n",
    "                out_images.append(self.transform(img))\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        \n",
    "        return out_images, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3df3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da66e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "zoom_mnist_dataset = Zoom_MNIST(root=\".\", train=False, transform=transform )\n",
    "images, label = zoom_mnist_dataset[0]\n",
    "dl = DataLoader(zoom_mnist_dataset, batch_size=5, num_workers=NUM_WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc07b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(2.8088) tensor(-0.4242)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-white')\n",
    "plt.figure(figsize=(10, 8))\n",
    "for j, img in enumerate(images):\n",
    "    print (img.shape)\n",
    "    print (img.max(), img.min())\n",
    "    plt.subplot(1, 3, j+1)\n",
    "    plt.imshow(img.numpy().squeeze())  # convert CHW -> HWC\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef57ace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7983)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Dirichlet(torch.tensor([10, 10]))\n",
    "m.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bab35db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.8215) tensor(-0.4242)\n",
      "tensor(2.7324) tensor(-0.4242)\n",
      "tensor(2.7706) tensor(-0.4242)\n",
      "tensor(2.7451) tensor(-0.4242)\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "tensor(2.7451) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAADBCAYAAADLnGp0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJdElEQVR4nO3dy49N2R4H8HWuUtEeHRFEBB3pHnablITQocSEGEgYkHTMpPUfIDHQhJggYkBElBgSExIGpooojyAGJD1tEkK84nGq6Ebdyb2dW6lVt52u/Tj18/nMfHOy9q/b3uWb5Sy7MTg4OJgAAAL7V90DAACUTeEBAMJTeACA8BQeACA8hQcACE/hAQDCU3hKdPXq1bRq1arU3d2dNm3alB4/flz3SFArzwQM5ZmoTsO/w1OOZrOZVq5cmY4cOZK6urpST09PunnzZjp+/Hjdo0EtPBMwlGeiWnZ4SnL9+vX0zTffpK6urpRSSj/99FO6du1aevv2bc2TQT08EzCUZ6JaCk9J7t+/n+bMmfPXrydNmpSmTp2aHjx4UONUUB/PBAzlmaiWwlOSgYGB1NnZOSTr7OxM/f39NU0E9fJMwFCeiWopPCWZOHFiajabQ7Jms5kmTpxY00RQL88EDOWZqJbCU5L58+en33///a9fv3jxIjWbzTRv3rz6hoIaeSZgKM9EtRSekixevDg9f/483bx5M6WU0qlTp9Ly5cvT5MmTa54M6uGZgKE8E9VyLL1Et27dSjt37kzNZjN9++23ad++fWnGjBl1jwW18UzAUJ6J6ig8AEB4/koLAAhP4QEAwlN4AIDwFB4AIDyFBwAIT+EBAMJTeACA8BQeACA8hQcACE/hAQDCU3gAgPAUHgAgPIUHAAhP4QEAwlN4AIDwFB4AIDyFBwAIT+EBAMJTeACA8BQeACA8hQcACE/hAQDCU3gAgPAUHgAgPIUHAAhP4QEAwlN4AIDwFB4AIDyFBwAIT+EBAMJTeACA8BQeACA8hQcACK+j7gEA4Es3MDCQzc+fP5/NlyxZks3nzp1b2EzR2OEBAMJTeACA8BQeACA8hQcACE/hAQDCawwODg7WPQQAfMkajUYh6zx58iSbz5w5s5D1xzI7PABAeAoPABCewgMAhKfwAADhKTwAQHjepQUAFVq7dm1pa+/YsSObHzt2rLRrjhV2eACA8BQeACA8hQcACE/hAQDCU3gAgPC8SwsARqG/vz+br1mzJpv39vaWOE3ew4cPs/ns2bMrnqQ+dngAgPAUHgAgPIUHAAhP4QEAwlN4AIDwnNIC2l6j0ShkHT/uKENR92cdOjs7s/n79+8rnqR8dngAgPAUHgAgPIUHAAhP4QEAwlN4AIDwOuoeAOC/nj17Vur6I53SGsunbKjOnj176h6hcH/88Uc2H+kdW48ePSpznFLZ4QEAwlN4AIDwFB4AIDyFBwAIT+EBAMLzLq0xxPuEiOLEiRPZfPPmzaVed9u2bdl87969pV6XGJzmS2lgYCCbT5gwoeJJWmeHBwAIT+EBAMJTeACA8BQeACA8hQcACM8prTa0f//+bD7SCZNW+S2nbmPltItn5cvUTvfnn3/+mc07Olp7FebChQuz+e3bt1ueKefNmzfZfPLkyYWsXwQ7PABAeAoPABCewgMAhKfwAADhKTwAQHhOadWonU4CpJRSX19fNl+yZEnFkxBJu93nRfBjM7ay79lLly4Ny5YtW1bqNT9+/JjNWz3t1ap2elbs8AAA4Sk8AEB4Cg8AEJ7CAwCEV+63lUgppXTu3Lm6R/gsS5cuzeafPn3K5hG/jMo/t379+sqv2d3dnc1PnjyZzXt6erL57t27W7ruSP+tZ86caWkdYjt9+nQ2L/sLyjnjxo3L5uvWrcvmZ8+eLXOcWtjhAQDCU3gAgPAUHgAgPIUHAAhP4QEAwvNqiQrUdZpppBMCGzduLGR9tw7/q8z7/OLFi9l8pFNarSpq9tycRc1IdVq9H0b6Wbthw4YixinV2rVrs/n58+cLWf/u3bvDsu+//76QtVtlhwcACE/hAQDCU3gAgPAUHgAgPIUHAAjPu7QKdvDgwdLWXr16dTa/cOFCS+uMdHKg1ZMJuc87ucVo1HX/NJvNbD5p0qSW1sm9f8gprfZ16NChQtbp6Gj/P0rfvXuXzYs6jTWSuk5k5djhAQDCU3gAgPAUHgAgPIUHAAhP4QEAwmv/r5aPMS9evBj1GgMDA9l8woQJo14b2kHU03yHDx8elhV1Eoji/frrr3WPULh79+5l8x9++KHU6y5durTU9YtghwcACE/hAQDCU3gAgPAUHgAgPIUHAAjPKa2C9fX1ffZnI55UGemEQDu9T4XR2bJlS90jFK7Vd2aNZPv27YWsA3+nt7c3m69YsaLaQf7jypUrtVy3FXZ4AIDwFB4AIDyFBwAIT+EBAMJTeACA8BqDEY8K1ajRaHz2Z+v6X//u3bts/tVXX416bbdTfF9//XU2f/PmzWevUfZ98urVq2w+derUUq/r/h9biriXU0rp559/zubHjh3L5keOHBmW3blzJ/vZly9fZvOzZ89+5nTFun79ejZftGhRxZO0zg4PABCewgMAhKfwAADhKTwAQHgKDwAQnlNaBWunU1ojvfOop6enkPUfPHgwLJs7d24ha9O+xsIprVaew39i3bp12fzMmTOlXpdibd68OZufOHGi4knGjrFcGezwAADhKTwAQHgKDwAQnsIDAISn8AAA4TmlVbAiTmn99ttv2XzTpk3Z/Pbt2599zSK5db5MP/74Yzbv6+ureJL6uPdj+PDhQzYfP358xZO0n6dPn2bz6dOnVzxJcezwAADhKTwAQHgKDwAQnsIDAISn8AAA4XXUPcCXrOz3/RTl06dPdY9AG+nt7c3mEU+2XLp0qe4RKFFHR/6PwK1bt2bzAwcOlDlOqS5evJjNu7u7qx2kRnZ4AIDwFB4AIDyFBwAIT+EBAMJTeACA8LxLq2A3btwYli1evLiGSVrnVmA0du3alc13795d7SD/x9GjR7P5L7/8UvEkjEX9/f3ZfMqUKdm8jhOuT548yeYzZ86seJL2Y4cHAAhP4QEAwlN4AIDwFB4AIDxfWq7Ay5cvs/m0adNKve7q1auz+YULF0q9LvydBQsWZPO7d++2tM7p06ezeVdXVzb/7rvvWlofRuP169fZ/PLly8OyhQsXZj87a9asQmf6ktnhAQDCU3gAgPAUHgAgPIUHAAhP4QEAwnNKCwAIzw4PABCewgMAhKfwAADhKTwAQHgKDwAQnsIDAISn8AAA4Sk8AEB4Cg8AEJ7CAwCEp/AAAOEpPABAeAoPABCewgMAhKfwAADhKTwAQHgKDwAQnsIDAISn8AAA4Sk8AEB4Cg8AEJ7CAwCEp/AAAOEpPABAeP8GnF1pmVErBSkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "batch_images, labels = next(iter(dl))\n",
    "\n",
    "for i, lable in enumerate(labels):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for j, images in enumerate( batch_images):\n",
    "        #logits = self(images)\n",
    "        #alpha = F.relu(logits) + 1\n",
    "        #preds = torch.argmax(alpha, dim=1)\n",
    "        #uncertainty = num_classes / alpha.sum(dim=1)\n",
    "        #drichlet_uncertainty = Dirichlet(alpha)\n",
    "        plt.subplot(1, len(batch_images), j+1)\n",
    "        print (images[i].max(), images[i].min())\n",
    "        plt.imshow(images[i].numpy().squeeze(), vmin=0, vmax=1)  # convert CHW -> HWC\n",
    "        plt.title(lable.item())\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a744c3",
   "metadata": {},
   "source": [
    "## MNIST Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72eabd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.loss = evidence_loss.edl_mse_loss\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomAffine(degrees=0,translate=(0.2,0.2),scale=(0.5,1.2)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define PyTorch model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, self.num_classes),\n",
    "        )\n",
    "        #self.model = resnet18(num_classes=10)\n",
    "        # Have ResNet model take in grayscale rather than RGB\n",
    "        #self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_0_accuracy = Accuracy()\n",
    "        self.test_1_accuracy = Accuracy()\n",
    "        self.test_2_accuracy = Accuracy()\n",
    "        self.test_accuracy = [self.test_0_accuracy,\n",
    "                             self.test_1_accuracy,\n",
    "                             self.test_2_accuracy]\n",
    "        \n",
    "        self.train_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        self.valid_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        \n",
    "        self.DS_combine = uncertain_fusion.DempsterSchaferCombine(self.num_classes)\n",
    "        self.mean_combine = uncertain_fusion.MeanUncertainty(self.num_classes)\n",
    "        self.sum_combine = uncertain_fusion.SumUncertainty(self.num_classes)\n",
    "        self.bayesian = uncertain_fusion.EffectiveProbability(confusion_matrix = np.ones((self.num_classes, \n",
    "                                                                                          self.num_classes)))\n",
    "        self.dampster = uncertain_fusion.EffectiveProbability(confusion_matrix = np.ones((self.num_classes, \n",
    "                                                                                          self.num_classes)),\n",
    "                                                                 fusion_type='dampster')\n",
    "        self.conv_1d = torch.nn.Sequential(\n",
    "                          torch.nn.Conv2d(in_channels=2*self.num_classes, \n",
    "                                       out_channels=self.num_classes, \n",
    "                                       kernel_size=1, \n",
    "                                       device=self.device),\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        self.DS_combine_accuracy = Accuracy()\n",
    "        self.mean_combine_accuracy = Accuracy()\n",
    "        self.sum_combine_accuracy = Accuracy()\n",
    "        self.bayes_combine_accuracy = Accuracy()\n",
    "        self.dampster_combine_accuracy = Accuracy()\n",
    "                                                                         \n",
    "        \n",
    "        self.fusion_methods = [self.DS_combine, \n",
    "                               self.mean_combine, \n",
    "                               self.sum_combine,\n",
    "                               self.bayesian ,\n",
    "                                self.dampster]\n",
    "        self.fusion_names = ['DS_combine', 'mean', 'sum', 'bayes', 'dampster']\n",
    "        self.combine_accuracy = [self.DS_combine_accuracy, \n",
    "                                 self.mean_combine_accuracy, \n",
    "                                 self.sum_combine_accuracy, \n",
    "                                 self.bayes_combine_accuracy,\n",
    "                                 self.dampster_combine_accuracy\n",
    "                                 ]\n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        \n",
    "    def logits_transformation(self, logits):\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        #logits = F.sigmoid(logits)\n",
    "        logits = logits * 200\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        ##Experiment with lgoits\n",
    "        logits = self.logits_transformation(logits)\n",
    "        #########################\n",
    "        #nllloss = F.nll_loss(F.log_softmax(logits, dim=1), y)\n",
    "        y = F.one_hot(y.to(torch.long), self.num_classes)\n",
    "        loss = evidence_loss.edl_mse_loss(logits, y, self.current_epoch, self.num_classes, 1)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        ##Experiment with lgoits\n",
    "        logits = self.logits_transformation(logits)\n",
    "        #########################\n",
    "        \n",
    "        y = F.one_hot(y.to(torch.long), self.num_classes)\n",
    "        loss = evidence_loss.edl_mse_loss(logits, y, self.current_epoch, self.num_classes, 1)\n",
    "        #loss = F.nll_loss(logits, y)\n",
    "        alpha = F.relu(logits) + 1\n",
    "        preds = torch.argmax(alpha, dim=1)\n",
    "        self.val_accuracy.update(preds, y.argmax(dim=1))\n",
    "        \n",
    "        self.train_cm(preds, y.argmax(dim=1))\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        \n",
    "        #self.logger.experiment.add_scalars('Accuracy', {'val': self.val_accuracy.compute()},self.global_step)\n",
    "        self.log(\"val/accuracy\", self.val_accuracy, prog_bar=True)\n",
    "        \n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # turn confusion matrix into a figure (Tensor cannot be logged as a scalar)\n",
    "        fig, ax = plt.subplots(figsize=(20,20))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=self.train_cm.compute().cpu().numpy())\n",
    "        disp.plot(ax=ax)\n",
    "        # log figure\n",
    "        self.logger.experiment.add_figure('val/epoch_confmat', fig, global_step=self.global_step)\n",
    "        \n",
    "        self.train_cm_numpy =  self.train_cm.compute().cpu().numpy()\n",
    "    \n",
    "        self.train_cm.reset()\n",
    "        self.bayesian = uncertain_fusion.EffectiveProbability(confusion_matrix = self.train_cm_numpy)\n",
    "        self.dampster = uncertain_fusion.EffectiveProbability(confusion_matrix = self.train_cm_numpy,\n",
    "                                                                 fusion_type='dampster')\n",
    "        #self.dampster = uncertain_fusion.EffectiveProbability(confusion_matrix = cm, fusion_type='dampster')\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        zoom_images = 3\n",
    "        threshold = 0.2\n",
    "        if batch_idx  % 10== 0:\n",
    "            fig = plt.figure(figsize=[6, 5])\n",
    "            fig, axs = plt.subplots(5, gridspec_kw={\"height_ratios\": [5, 1, 1, 10, 10]})\n",
    "            rimgs = np.zeros((28, 28 * zoom_images))\n",
    "            scores = np.zeros((1, num_classes))\n",
    "            fusion_scores = np.zeros((1, num_classes))\n",
    "            classifications = []\n",
    "            fusion_output = []\n",
    "            lu = []\n",
    "            fusion_lu = []\n",
    "            lp = []\n",
    "            fusion_lp = []\n",
    "            ldeg = []\n",
    "            \n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            ##Experiment with lgoits\n",
    "            logits = self.logits_transformation(logits)\n",
    "            #########################\n",
    "            loss = evidence_loss.edl_mse_loss(logits, labels, self.current_epoch, self.num_classes, 1)\n",
    "            #loss = F.nll_loss(logits, labels)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            probs = alpha /  torch.sum(alpha, dim=1, keepdim=True)\n",
    "            if batch_idx  % 10 == 0:\n",
    "                rimgs[:, j*28:(j+1)*28] = batch_images[0].detach().cpu().numpy().reshape(28,28)\n",
    "                classifications.append(preds[0].item())\n",
    "                scores += probs[0].detach().cpu().numpy() >= threshold\n",
    "                lu.append(uncertainty[0].item())\n",
    "                ldeg.append(j)\n",
    "                lp.append(probs[0].tolist())\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(\"val/fusion/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            \n",
    "            if prev_alpha is not None:\n",
    "                for fuse, name, fuse_acc in zip(self.fusion_methods, \n",
    "                                                  self.fusion_names, \n",
    "                                                  self.combine_accuracy):\n",
    "                    \n",
    "                    fused_alpha = fuse(prev_alpha[name].to(self.device).to(torch.float), alpha)\n",
    "                    fused_alpha = F.relu(fused_alpha) + 1\n",
    "                    preds = torch.argmax(fused_alpha, dim=1)\n",
    "                    uncertainty = self.num_classes / torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "                    probs = fused_alpha /  torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "                    fuse_acc.update(preds.to('cpu'), labels.argmax(dim=1).to('cpu'))\n",
    "                    \n",
    "                    self.log(\"val/fusion/accuracy/\"+name, fuse_acc, prog_bar=True)\n",
    "            \n",
    "                    prev_alpha[name] = fused_alpha\n",
    "                if batch_idx  % 10== 0:\n",
    "                    fusion_output.append(preds[0].item())\n",
    "                    fusion_scores += probs[0].detach().cpu().numpy() >= threshold\n",
    "                    print (\"Uncertainty of 0 img\", uncertainty[0].item() )\n",
    "                    print (\"Fused alpha  of 0 img\", fused_alpha[0])\n",
    "                    fusion_lu.append(uncertainty[0].item())\n",
    "                    fusion_lp.append(probs[0].tolist())\n",
    "                   \n",
    "            else:\n",
    "                prev_alpha={}\n",
    "                for name in self.fusion_names:\n",
    "                    prev_alpha[name] = alpha\n",
    "                    \n",
    "                if batch_idx % 10 == 0:\n",
    "                    fusion_output.append(\"-\")\n",
    "                    fusion_scores += probs[0].detach().cpu().numpy() >= threshold\n",
    "                    #fusion_scores += np.zeros((1, num_classes)) >= threshold\n",
    "                    fusion_lu.append(uncertainty[0].item())\n",
    "                    fusion_lp.append(probs[0].tolist())\n",
    "                   \n",
    "                    \n",
    "        if batch_idx % 10 == 0:\n",
    "            #logits = self(images)\n",
    "            #alpha = F.relu(logits) + 1\n",
    "            #preds = torch.argmax(alpha, dim=1)\n",
    "            #uncertainty = num_classes / alpha.sum(dim=1)\n",
    "            #drichlet_uncertainty = Dirichlet(alpha)\n",
    "            #plt.subplot(1, len(list_images), j+1)\n",
    "            axs[0].imshow(rimgs, vmin=0, vmax=1)  # convert CHW -> HWC\n",
    "            plt.title(labels[0].argmax().to('cpu').item())\n",
    "            axs[0].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(classifications)\n",
    "            axs[1].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[1].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(fusion_output)\n",
    "            axs[2].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[2].axis(\"off\")\n",
    "            \n",
    "            axs[3].plot(ldeg, lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            labels = np.arange(10)[scores[0].astype(bool)]\n",
    "            lp = np.array(lp)[:, labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            labels = labels.tolist()\n",
    "            for i in range(len(labels)):\n",
    "                print (\"plot  lp \", lp[:, i])\n",
    "                axs[3].plot(ldeg, lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[3].set_ylim([0, 1])\n",
    "            axs[3].legend(labels)\n",
    "                \n",
    "            fusion_labels = np.arange(10)[fusion_scores[0].astype(bool)]\n",
    "            fusion_lp = np.array(fusion_lp)[:, fusion_labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            fusion_labels = fusion_labels.tolist()\n",
    "            print (\"fusion_labels \", fusion_labels)\n",
    "            for i in range(len(fusion_labels)):\n",
    "                print (\"plot fusion lp \", fusion_lp[:, i])\n",
    "                axs[4].plot(ldeg, fusion_lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[4].plot(ldeg, fusion_lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            axs[4].set_ylim([0, 1])\n",
    "            axs[4].legend(labels)\n",
    "            axs[4].set_xlim([0, 1])        \n",
    "            \n",
    "            \n",
    "            # log figure\n",
    "            self.logger.experiment.add_figure('sample', fig, global_step=self.global_step)\n",
    "\n",
    "            \n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"test/loss\", loss, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=1, eta_min=1e-4, last_epoch=-1)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        \n",
    "          \n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "        Zoom_MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            #self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "            self.mnist_zoom = Zoom_MNIST(self.data_dir, train=False, transform=self.val_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e5179f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized fusion type :  bayes\n",
      "Initialized fusion type :  dampster\n"
     ]
    }
   ],
   "source": [
    "model = LitMNIST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b4ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9d9c586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                      | Type                   | Params\n",
      "----------------------------------------------------------------------\n",
      "0  | model                     | Sequential             | 55.1 K\n",
      "1  | val_accuracy              | Accuracy               | 0     \n",
      "2  | test_0_accuracy           | Accuracy               | 0     \n",
      "3  | test_1_accuracy           | Accuracy               | 0     \n",
      "4  | test_2_accuracy           | Accuracy               | 0     \n",
      "5  | train_cm                  | ConfusionMatrix        | 0     \n",
      "6  | valid_cm                  | ConfusionMatrix        | 0     \n",
      "7  | DS_combine                | DempsterSchaferCombine | 0     \n",
      "8  | mean_combine              | MeanUncertainty        | 0     \n",
      "9  | sum_combine               | SumUncertainty         | 0     \n",
      "10 | bayesian                  | EffectiveProbability   | 0     \n",
      "11 | dampster                  | EffectiveProbability   | 0     \n",
      "12 | conv_1d                   | Sequential             | 210   \n",
      "13 | DS_combine_accuracy       | Accuracy               | 0     \n",
      "14 | mean_combine_accuracy     | Accuracy               | 0     \n",
      "15 | sum_combine_accuracy      | Accuracy               | 0     \n",
      "16 | bayes_combine_accuracy    | Accuracy               | 0     \n",
      "17 | dampster_combine_accuracy | Accuracy               | 0     \n",
      "----------------------------------------------------------------------\n",
      "55.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.3 K    Total params\n",
      "0.221     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized fusion type :  bayes\n",
      "Initialized fusion type :  dampster\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf9e0eef6214176b3cb250fa6476459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized fusion type :  bayes\n",
      "Initialized fusion type :  dampster\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger('./lightning_logs/', name=LOGGER_NAME)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    max_epochs=2,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    #logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    check_val_every_n_epoch=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f126467",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model, 'zoom_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "381a8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15169b62a564e01aa7dfba9e0622997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "fusion_labels  []\n",
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "fusion_labels  []\n",
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "fusion_labels  []\n",
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "0.909090907859408 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "fusion_labels  []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">          Test metric           </span>┃<span style=\"font-weight: bold\">          DataLoader 0          </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           test/loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       1.3979148864746094       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val/fusion/accuracy/0      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09790000319480896       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val/fusion/accuracy/1      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09799999743700027       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val/fusion/accuracy/2      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09799999743700027       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> val/fusion/accuracy/DS_combine </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09790000319480896       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   val/fusion/accuracy/bayes    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09794999659061432       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  val/fusion/accuracy/dampster  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09794999659061432       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val/fusion/accuracy/mean    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09799999743700027       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val/fusion/accuracy/sum     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.09794999659061432       </span>│\n",
       "└────────────────────────────────┴────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         Test metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m         DataLoader 0         \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          test/loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      1.3979148864746094      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val/fusion/accuracy/0     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09790000319480896      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val/fusion/accuracy/1     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09799999743700027      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val/fusion/accuracy/2     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09799999743700027      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mval/fusion/accuracy/DS_combine\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09790000319480896      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  val/fusion/accuracy/bayes   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09794999659061432      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m val/fusion/accuracy/dampster \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09794999659061432      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val/fusion/accuracy/mean   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09799999743700027      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val/fusion/accuracy/sum    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.09794999659061432      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└────────────────────────────────┴────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val/fusion/accuracy/0': 0.09790000319480896,\n",
       "  'val/fusion/accuracy/1': 0.09799999743700027,\n",
       "  'val/fusion/accuracy/DS_combine': 0.09790000319480896,\n",
       "  'val/fusion/accuracy/mean': 0.09799999743700027,\n",
       "  'val/fusion/accuracy/sum': 0.09794999659061432,\n",
       "  'val/fusion/accuracy/bayes': 0.09794999659061432,\n",
       "  'val/fusion/accuracy/dampster': 0.09794999659061432,\n",
       "  'val/fusion/accuracy/2': 0.09799999743700027,\n",
       "  'test/loss': 1.3979148864746094}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.model = torch.load( 'zoom_mnist.pt')\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02dbc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bec9ec4",
   "metadata": {},
   "source": [
    "## 1 D fusion Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91bf6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDLitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.loss = evidence_loss.edl_mse_loss\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomAffine(degrees=0,translate=(0.2,0.2),scale=(0.5,1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.model = resnet18( num_classes=10)\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Have ResNet model take in grayscale rather than RGB\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_0_accuracy = Accuracy()\n",
    "        self.test_1_accuracy = Accuracy()\n",
    "        self.test_2_accuracy = Accuracy()\n",
    "        self.test_accuracy = [self.test_0_accuracy,\n",
    "                             self.test_1_accuracy,\n",
    "                             self.test_2_accuracy]\n",
    "        \n",
    "        self.train_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        self.valid_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        \n",
    "        \n",
    "        self.fc_fusion = torch.nn.Linear(2*self.num_classes, self.num_classes,device=self.device )\n",
    "        \n",
    "        \n",
    "        self.one_d_accuray = Accuracy()\n",
    "        self.two_d_accuracy = Accuracy()\n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def common_epoch_step(self, batch, batch_idx, stage):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(stage+\"/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            \n",
    "            if prev_alpha is not None:\n",
    "\n",
    "                fused_alpha = torch.concat((prev_alpha.to(self.device).to(torch.float),alpha), dim=1)\n",
    "                fused_alpha = self.fc_fusion(fused_alpha)\n",
    "                preds = torch.argmax(fused_alpha, dim=1)\n",
    "                \n",
    "                #self.logger.experiment.add_scalars('Accuracy', \n",
    "                #                                   {'fusion'+name: fuse_acc.compute()},\n",
    "                #                                   self.global_step) \n",
    "            \n",
    "                prev_alpha = fused_alpha\n",
    "                \n",
    "\n",
    "            else:\n",
    "                prev_alpha = alpha\n",
    "            \n",
    "        \n",
    "        \n",
    "                    \n",
    "            \n",
    "        loss = evidence_loss.edl_mse_loss(fused_alpha, labels, self.current_epoch, self.num_classes, 1)\n",
    "        self.one_d_accuray.update(preds, labels.argmax(dim=1))\n",
    "        self.log(stage+\"/one D accuracy\", self.one_d_accuray, prog_bar=True)\n",
    "                \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(stage+\"/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'valid')\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.AdamW(self.fc_fusion.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=1, eta_min=1e-4, last_epoch=-1)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        \n",
    "          \n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        Zoom_MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.mnist_zoom_train = Zoom_MNIST(self.data_dir, train=True, transform=self.val_transform)\n",
    "            self.mnist_zoom_valid = Zoom_MNIST(self.data_dir, train=False, transform=self.val_transform)\n",
    "\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_valid, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54931b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56051aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type            | Params\n",
      "----------------------------------------------------\n",
      "0 | model           | ResNet          | 11.2 M\n",
      "1 | val_accuracy    | Accuracy        | 0     \n",
      "2 | test_0_accuracy | Accuracy        | 0     \n",
      "3 | test_1_accuracy | Accuracy        | 0     \n",
      "4 | test_2_accuracy | Accuracy        | 0     \n",
      "5 | train_cm        | ConfusionMatrix | 0     \n",
      "6 | valid_cm        | ConfusionMatrix | 0     \n",
      "7 | fc_fusion       | Linear          | 210   \n",
      "8 | one_d_accuray   | Accuracy        | 0     \n",
      "9 | two_d_accuracy  | Accuracy        | 0     \n",
      "----------------------------------------------------\n",
      "210       Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.702    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399f6b31d7ec499a8429ef185d6f8867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = OneDLitMNIST()\n",
    "model.model = torch.load('zoom_mnist.pt')\n",
    "model.model.eval()\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "logger = TensorBoardLogger('./lightning_logs/', name=LOGGER_NAME)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    max_epochs=2,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    #logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    check_val_every_n_epoch=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "809bd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DampsterLitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.loss = evidence_loss.edl_mse_loss\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomAffine(degrees=0,translate=(0.2,0.2),scale=(0.5,1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.model = resnet18( num_classes=10)\n",
    "            \n",
    "        # Have ResNet model take in grayscale rather than RGB\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_0_accuracy = Accuracy()\n",
    "        self.test_1_accuracy = Accuracy()\n",
    "        self.test_2_accuracy = Accuracy()\n",
    "        self.test_accuracy = [self.test_0_accuracy,\n",
    "                             self.test_1_accuracy,\n",
    "                             self.test_2_accuracy]\n",
    "        \n",
    "        self.train_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        self.valid_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        \n",
    "        \n",
    "        #self.fc_fusion = torch.nn.Linear(2*self.num_classes, self.num_classes,device=self.device )\n",
    "        self.DS_combine = uncertain_fusion.DempsterSchaferCombine(self.num_classes)\n",
    "        \n",
    "        self.one_d_accuray = Accuracy()\n",
    "        self.two_d_accuracy = Accuracy()\n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def common_epoch_step(self, batch, batch_idx, stage):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(stage+\"/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            \n",
    "            if prev_alpha is not None:\n",
    "\n",
    "                #fused_alpha = torch.concat((prev_alpha.to(self.device).to(torch.float),alpha), dim=1)\n",
    "                fused_alpha = self.DS_combine(prev_alpha.to(self.device).to(torch.float), alpha)\n",
    "                preds = torch.argmax(fused_alpha, dim=1)\n",
    "                \n",
    "                #self.logger.experiment.add_scalars('Accuracy', \n",
    "                #                                   {'fusion'+name: fuse_acc.compute()},\n",
    "                #                                   self.global_step) \n",
    "            \n",
    "                prev_alpha = fused_alpha\n",
    "                \n",
    "\n",
    "            else:\n",
    "                prev_alpha = alpha\n",
    "            \n",
    "           \n",
    "        loss = evidence_loss.edl_mse_loss(fused_alpha, labels, self.current_epoch, self.num_classes, 1)\n",
    "        self.one_d_accuray.update(preds, labels.argmax(dim=1))\n",
    "        self.log(stage+\"/one D accuracy\", self.one_d_accuray, prog_bar=True)\n",
    "                \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(stage+\"/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'valid')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        zoom_images = 3\n",
    "        threshold = 0.2\n",
    "        if batch_idx  % 10== 0:\n",
    "            fig = plt.figure(figsize=[6, 5])\n",
    "            fig, axs = plt.subplots(5, gridspec_kw={\"height_ratios\": [5, 1, 1, 10, 10]})\n",
    "            rimgs = np.zeros((28, 28 * zoom_images))\n",
    "            scores = np.zeros((1, num_classes))\n",
    "            fusion_scores = np.zeros((1, num_classes))\n",
    "            classifications = []\n",
    "            fusion_output = []\n",
    "            lu = []\n",
    "            fusion_lu = []\n",
    "            lp = []\n",
    "            fusion_lp = []\n",
    "            ldeg = []\n",
    "            \n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            loss = evidence_loss.edl_mse_loss(logits, labels, self.current_epoch, self.num_classes, 5)\n",
    "            #loss = F.nll_loss(logits, labels)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            if 0 == j:#Find the image with mx uncertainty in the first image only\n",
    "                max_uncertain_args = torch.argmax(uncertainty)\n",
    "            probs = alpha /  torch.sum(alpha, dim=1, keepdim=True)\n",
    "            if batch_idx  % 10 == 0:\n",
    "                \n",
    "                rimgs[:, j*28:(j+1)*28] = batch_images[max_uncertain_args].detach().cpu().numpy().reshape(28,28)\n",
    "                classifications.append(preds[max_uncertain_args].item())\n",
    "                scores += probs[max_uncertain_args].detach().cpu().numpy() >= threshold\n",
    "                lu.append(uncertainty[max_uncertain_args].item())\n",
    "                ldeg.append(j)\n",
    "                lp.append(probs[max_uncertain_args].tolist())\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(\"test/fusion/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            if prev_alpha is not None:\n",
    "                \n",
    "                    \n",
    "                fused_alpha = self.DS_combine(prev_alpha.to(self.device).to(torch.float), alpha)\n",
    "                fused_alpha = F.relu(fused_alpha) + 1\n",
    "                preds = torch.argmax(fused_alpha, dim=1)\n",
    "                uncertainty = self.num_classes / torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "                probs = fused_alpha /  torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "\n",
    "                prev_alpha = fused_alpha\n",
    "                if batch_idx  % 10== 0:\n",
    "                    fusion_output.append(preds[max_uncertain_args].item())\n",
    "                    fusion_scores += probs[max_uncertain_args].detach().cpu().numpy() >= threshold\n",
    "                    print (uncertainty[max_uncertain_args].item(), fused_alpha[max_uncertain_args], )\n",
    "                    fusion_lu.append(uncertainty[max_uncertain_args].item())\n",
    "                    fusion_lp.append(probs[max_uncertain_args].tolist())\n",
    "                   \n",
    "            else:\n",
    "                prev_alpha = alpha\n",
    "                uncertainty = self.num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "                probs = alpha /  torch.sum(alpha, dim=1, keepdim=True)\n",
    "    \n",
    "                if batch_idx % 10 == 0:\n",
    "                    fusion_output.append(\"-\")\n",
    "                    fusion_scores += probs[max_uncertain_args].detach().cpu().numpy() >= threshold\n",
    "                    #fusion_scores += np.zeros((1, num_classes)) >= threshold\n",
    "                    fusion_lu.append(uncertainty[0].item())\n",
    "                    fusion_lp.append(probs[max_uncertain_args].tolist())\n",
    "                    \n",
    "        self.one_d_accuray.update(preds, labels.argmax(dim=1))\n",
    "        self.log(\"test/fusion/accuracy/one_D_accuracy\", self.one_d_accuray, prog_bar=True)\n",
    "                   \n",
    "                    \n",
    "        if batch_idx % 10 == 0:\n",
    "            #logits = self(images)\n",
    "            #alpha = F.relu(logits) + 1\n",
    "            #preds = torch.argmax(alpha, dim=1)\n",
    "            #uncertainty = num_classes / alpha.sum(dim=1)\n",
    "            #drichlet_uncertainty = Dirichlet(alpha)\n",
    "            #plt.subplot(1, len(list_images), j+1)\n",
    "            axs[0].imshow(rimgs, vmin=0, vmax=1)  # convert CHW -> HWC\n",
    "            plt.title(labels[max_uncertain_args].argmax().to('cpu').item())\n",
    "            axs[0].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(classifications)\n",
    "            axs[1].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[1].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(fusion_output)\n",
    "            axs[2].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[2].axis(\"off\")\n",
    "            \n",
    "            axs[3].plot(ldeg, lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            labels = np.arange(10)[scores[0].astype(bool)]\n",
    "            lp = np.array(lp)[:, labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            labels = labels.tolist()\n",
    "            for i in range(len(labels)):\n",
    "                print (\"plot  lp \", lp[:, i])\n",
    "                axs[3].plot(ldeg, lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[3].set_ylim([0, 1])\n",
    "            axs[3].legend(labels)\n",
    "                \n",
    "            fusion_labels = np.arange(10)[fusion_scores[0].astype(bool)]\n",
    "            fusion_lp = np.array(fusion_lp)[:, fusion_labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            fusion_labels = fusion_labels.tolist()\n",
    "            print (\"fusion_labels \", fusion_labels)\n",
    "            for i in range(len(fusion_labels)):\n",
    "                print (\"plot fusion lp \", fusion_lp[:, i])\n",
    "                axs[4].plot(ldeg, fusion_lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[4].plot(ldeg, fusion_lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            axs[4].set_ylim([0, 1])\n",
    "            axs[4].legend(labels)\n",
    "            axs[4].set_xlim([0, 1])        \n",
    "            \n",
    "            \n",
    "            # log figure\n",
    "            self.logger.experiment.add_figure('sample_'+str(batch_idx), fig, global_step=self.global_step)\n",
    "\n",
    "            \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=1, eta_min=1e-4, last_epoch=-1)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        \n",
    "          \n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        Zoom_MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print (\"setup stage \", stage)\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        #if stage == \"fit\" or stage is None:\n",
    "        self.mnist_zoom_train = Zoom_MNIST(self.data_dir, train=True, transform=self.val_transform)\n",
    "        \n",
    "        self.mnist_zoom_valid = Zoom_MNIST(self.data_dir, train=False, transform=self.val_transform)\n",
    "\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_valid, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_valid, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43854079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model           | ResNet                 | 11.2 M\n",
      "1 | val_accuracy    | Accuracy               | 0     \n",
      "2 | test_0_accuracy | Accuracy               | 0     \n",
      "3 | test_1_accuracy | Accuracy               | 0     \n",
      "4 | test_2_accuracy | Accuracy               | 0     \n",
      "5 | train_cm        | ConfusionMatrix        | 0     \n",
      "6 | valid_cm        | ConfusionMatrix        | 0     \n",
      "7 | DS_combine      | DempsterSchaferCombine | 0     \n",
      "8 | one_d_accuray   | Accuracy               | 0     \n",
      "9 | two_d_accuracy  | Accuracy               | 0     \n",
      "-----------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.701    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup stage  TrainerFn.FITTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8571b7bf7d4375924b76c0ea605420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DampsterLitMNIST()\n",
    "#model.model = torch.load('zoom_mnist.pt')\n",
    "\n",
    "logger = TensorBoardLogger('./lightning_logs/', name=LOGGER_NAME)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    max_epochs=2,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    #logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    check_val_every_n_epoch=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5b779cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup stage  TrainerFn.TESTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dca7fec59624940ad0cda6f988bfbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2721967399120331 tensor([ 2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,\n",
      "         2.0000, 18.7381], device='cuda:0')\n",
      "0.08796845376491547 tensor([ 3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000,\n",
      "         3.0000, 86.6771], device='cuda:0')\n",
      "plot  lp  [0.1        0.66340208 0.73632145]\n",
      "fusion_labels  [9]\n",
      "plot fusion lp  [0.1        0.51004589 0.76248515]\n",
      "0.2080220878124237 tensor([ 2.0000,  2.0000,  2.0000, 30.0718,  2.0000,  2.0000,  2.0000,  2.0000,\n",
      "         2.0000,  2.0000], device='cuda:0')\n",
      "0.06647595018148422 tensor([  3.0000,   3.0000,   3.0000, 123.4303,   3.0000,   3.0000,   3.0000,\n",
      "          3.0000,   3.0000,   3.0000], device='cuda:0')\n",
      "plot  lp  [0.1        0.76360464 0.73244661]\n",
      "fusion_labels  [3]\n",
      "plot fusion lp  [0.1        0.62556022 0.82051492]\n",
      "0.21273541450500488 tensor([ 2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000, 29.0067,\n",
      "         2.0000,  2.0000], device='cuda:0')\n",
      "0.06737329810857773 tensor([  3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,\n",
      "        121.4268,   3.0000,   3.0000], device='cuda:0')\n",
      "plot  lp  [0.1        0.75680107 0.73571098]\n",
      "fusion_labels  [7]\n",
      "plot fusion lp  [0.1        0.61707628 0.81809205]\n",
      "0.16570894420146942 tensor([ 2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,\n",
      "         2.0000, 42.3468], device='cuda:0')\n",
      "0.056221503764390945 tensor([  3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,\n",
      "          3.0000,   3.0000, 150.8679], device='cuda:0')\n",
      "plot  lp  [0.1        0.82123977 0.70911622]\n",
      "fusion_labels  [9]\n",
      "plot fusion lp  [0.1        0.70172387 0.84820193]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">             Test metric             </span>┃<span style=\"font-weight: bold\">            DataLoader 0             </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/fusion/accuracy/0        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.6639999747276306          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/fusion/accuracy/1        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.9383999705314636          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/fusion/accuracy/2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.9682999849319458          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test/fusion/accuracy/one_D_accuracy </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.9778000116348267          </span>│\n",
       "└─────────────────────────────────────┴─────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m            Test metric            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m           DataLoader 0            \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/fusion/accuracy/0       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.6639999747276306         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/fusion/accuracy/1       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.9383999705314636         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/fusion/accuracy/2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.9682999849319458         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest/fusion/accuracy/one_D_accuracy\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.9778000116348267         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────────────┴─────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/fusion/accuracy/0': 0.6639999747276306,\n",
       "  'test/fusion/accuracy/1': 0.9383999705314636,\n",
       "  'test/fusion/accuracy/2': 0.9682999849319458,\n",
       "  'test/fusion/accuracy/one_D_accuracy': 0.9778000116348267}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f2abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
