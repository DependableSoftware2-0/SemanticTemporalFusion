{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5096fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger,CSVLogger\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import evidence_loss\n",
    "import uncertain_fusion\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count()/2)\n",
    "LOGGER_NAME = 'ZOOM_MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15b2aef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81175932, 0.18824068])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.dirichlet([30,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02bf8a",
   "metadata": {},
   "source": [
    "## ZOOM MNIST Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc58da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Zoom_MNIST(MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(Zoom_MNIST, self).__init__(root,  train=train,\n",
    "                                         download=download,\n",
    "                                         transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "        self.zoom_images = 3\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_zoom_images(img):\n",
    "        '''\n",
    "        returns list of images \n",
    "        '''\n",
    "        original_size = 28\n",
    "        \n",
    "        #Randomly select 3 image sizes by using dirichlet stick breaking\n",
    "        image_sizes = (np.random.dirichlet([30,10,10])*28).astype(int)\n",
    "        #image_sizes = (np.random.dirichlet([30,10])*28).astype(int)\n",
    "        size = 0\n",
    "        x = int(np.random.rand() * 28)\n",
    "        y = int(np.random.rand() * 28)\n",
    "        \n",
    "        output_images = []\n",
    "\n",
    "        for i, image_size in enumerate(image_sizes):\n",
    "            size +=image_size\n",
    "            if (x + size)>original_size:\n",
    "                x  = x - (x + size - original_size)\n",
    "            if (y + size)>original_size:\n",
    "                y = y - (y + size - original_size)\n",
    "            canvas = np.zeros((original_size, original_size), dtype=np.uint8)\n",
    "            small_img = cv2.resize(img.numpy().reshape(original_size,original_size, 1), (size,size))\n",
    "            canvas[ y:y+small_img.shape[0], x:x+small_img.shape[1]] = small_img\n",
    "            output_images.append(canvas.reshape(original_size,original_size))\n",
    "        return output_images\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        images = self.create_zoom_images(img)\n",
    "        \n",
    "        out_images=[]\n",
    "        for img in images:\n",
    "            pil_img = Image.fromarray(img, mode='L')\n",
    "\n",
    "            if self.transform is not None:\n",
    "                out_images.append(self.transform(img))\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        \n",
    "        return out_images, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3df3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0da66e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "zoom_mnist_dataset = Zoom_MNIST(root=\".\", train=False, transform=transform )\n",
    "images, label = zoom_mnist_dataset[0]\n",
    "dl = DataLoader(zoom_mnist_dataset, batch_size=5, num_workers=NUM_WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc07b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(2.8088) tensor(-0.4242)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-white')\n",
    "plt.figure(figsize=(10, 8))\n",
    "for j, img in enumerate(images):\n",
    "    print (img.shape)\n",
    "    print (img.max(), img.min())\n",
    "    plt.subplot(1, 3, j+1)\n",
    "    plt.imshow(img.numpy().squeeze())  # convert CHW -> HWC\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef57ace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7983)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Dirichlet(torch.tensor([10, 10]))\n",
    "m.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bab35db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8088) tensor(-0.4242)\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.8215) tensor(-0.4242)\n",
      "tensor(2.7706) tensor(-0.4242)\n",
      "tensor(2.7451) tensor(-0.4242)\n",
      "tensor(2.8088) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n",
      "tensor(2.7960) tensor(-0.4242)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "batch_images, labels = next(iter(dl))\n",
    "\n",
    "for i, lable in enumerate(labels):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for j, images in enumerate( batch_images):\n",
    "        #logits = self(images)\n",
    "        #alpha = F.relu(logits) + 1\n",
    "        #preds = torch.argmax(alpha, dim=1)\n",
    "        #uncertainty = num_classes / alpha.sum(dim=1)\n",
    "        #drichlet_uncertainty = Dirichlet(alpha)\n",
    "        plt.subplot(1, len(batch_images), j+1)\n",
    "        print (images[i].max(), images[i].min())\n",
    "        plt.imshow(images[i].numpy().squeeze(), vmin=0, vmax=1)  # convert CHW -> HWC\n",
    "        plt.title(lable.item())\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a744c3",
   "metadata": {},
   "source": [
    "## MNIST Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72eabd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.loss = evidence_loss.edl_mse_loss\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomAffine(degrees=0,translate=(0.2,0.2),scale=(0.5,1.2)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define PyTorch model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, self.num_classes),\n",
    "        )\n",
    "        self.model = resnet18(num_classes=10)\n",
    "        # Have ResNet model take in grayscale rather than RGB\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_0_accuracy = Accuracy()\n",
    "        self.test_1_accuracy = Accuracy()\n",
    "        self.test_2_accuracy = Accuracy()\n",
    "        self.test_accuracy = [self.test_0_accuracy,\n",
    "                             self.test_1_accuracy,\n",
    "                             self.test_2_accuracy]\n",
    "        \n",
    "        self.train_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        self.valid_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        \n",
    "        self.DS_combine = uncertain_fusion.DempsterSchaferCombine(self.num_classes)\n",
    "        self.mean_combine = uncertain_fusion.MeanUncertainty(self.num_classes)\n",
    "        self.sum_combine = uncertain_fusion.SumUncertainty(self.num_classes)\n",
    "        self.bayesian = uncertain_fusion.EffectiveProbability(confusion_matrix = np.ones((self.num_classes, \n",
    "                                                                                          self.num_classes)))\n",
    "        self.dampster = uncertain_fusion.EffectiveProbability(confusion_matrix = np.ones((self.num_classes, \n",
    "                                                                                          self.num_classes)),\n",
    "                                                                 fusion_type='dampster')\n",
    "        self.conv_1d = torch.nn.Sequential(\n",
    "                          torch.nn.Conv2d(in_channels=2*self.num_classes, \n",
    "                                       out_channels=self.num_classes, \n",
    "                                       kernel_size=1, \n",
    "                                       device=self.device),\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        self.DS_combine_accuracy = Accuracy()\n",
    "        self.mean_combine_accuracy = Accuracy()\n",
    "        self.sum_combine_accuracy = Accuracy()\n",
    "        self.bayes_combine_accuracy = Accuracy()\n",
    "        self.dampster_combine_accuracy = Accuracy()\n",
    "                                                                         \n",
    "        \n",
    "        self.fusion_methods = [self.DS_combine, \n",
    "                               self.mean_combine, \n",
    "                               self.sum_combine,\n",
    "                               self.bayesian ,\n",
    "                                self.dampster]\n",
    "        self.fusion_names = ['DS_combine', 'mean', 'sum', 'bayes', 'dampster']\n",
    "        self.combine_accuracy = [self.DS_combine_accuracy, \n",
    "                                 self.mean_combine_accuracy, \n",
    "                                 self.sum_combine_accuracy, \n",
    "                                 self.bayes_combine_accuracy,\n",
    "                                 self.dampster_combine_accuracy\n",
    "                                 ]\n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        #nllloss = F.nll_loss(F.log_softmax(logits, dim=1), y)\n",
    "        y = F.one_hot(y.to(torch.long), self.num_classes)\n",
    "        loss = evidence_loss.edl_mse_loss(logits, y, self.current_epoch, self.num_classes, 1)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        y = F.one_hot(y.to(torch.long), self.num_classes)\n",
    "        loss = evidence_loss.edl_mse_loss(logits, y, self.current_epoch, self.num_classes, 1)\n",
    "        #loss = F.nll_loss(logits, y)\n",
    "        alpha = F.relu(logits) + 1\n",
    "        preds = torch.argmax(alpha, dim=1)\n",
    "        self.val_accuracy.update(preds, y.argmax(dim=1))\n",
    "        \n",
    "        self.train_cm(preds, y.argmax(dim=1))\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        \n",
    "        #self.logger.experiment.add_scalars('Accuracy', {'val': self.val_accuracy.compute()},self.global_step)\n",
    "        self.log(\"val/accuracy\", self.val_accuracy, prog_bar=True)\n",
    "        \n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # turn confusion matrix into a figure (Tensor cannot be logged as a scalar)\n",
    "        fig, ax = plt.subplots(figsize=(20,20))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=self.train_cm.compute().cpu().numpy())\n",
    "        disp.plot(ax=ax)\n",
    "        # log figure\n",
    "        self.logger.experiment.add_figure('val/epoch_confmat', fig, global_step=self.global_step)\n",
    "        \n",
    "        self.train_cm_numpy =  self.train_cm.compute().cpu().numpy()\n",
    "    \n",
    "        self.train_cm.reset()\n",
    "        self.bayesian = uncertain_fusion.EffectiveProbability(confusion_matrix = self.train_cm_numpy)\n",
    "        self.dampster = uncertain_fusion.EffectiveProbability(confusion_matrix = self.train_cm_numpy,\n",
    "                                                                 fusion_type='dampster')\n",
    "        #self.dampster = uncertain_fusion.EffectiveProbability(confusion_matrix = cm, fusion_type='dampster')\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        zoom_images = 3\n",
    "        threshold = 0.2\n",
    "        if batch_idx  % 10== 0:\n",
    "            fig = plt.figure(figsize=[6, 5])\n",
    "            fig, axs = plt.subplots(5, gridspec_kw={\"height_ratios\": [5, 1, 1, 10, 10]})\n",
    "            rimgs = np.zeros((28, 28 * zoom_images))\n",
    "            scores = np.zeros((1, num_classes))\n",
    "            fusion_scores = np.zeros((1, num_classes))\n",
    "            classifications = []\n",
    "            fusion_output = []\n",
    "            lu = []\n",
    "            fusion_lu = []\n",
    "            lp = []\n",
    "            fusion_lp = []\n",
    "            ldeg = []\n",
    "            \n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            loss = evidence_loss.edl_mse_loss(logits, labels, self.current_epoch, self.num_classes, 5)\n",
    "            #loss = F.nll_loss(logits, labels)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            probs = alpha /  torch.sum(alpha, dim=1, keepdim=True)\n",
    "            if batch_idx  % 10 == 0:\n",
    "                rimgs[:, j*28:(j+1)*28] = batch_images[0].detach().cpu().numpy().reshape(28,28)\n",
    "                classifications.append(preds[0].item())\n",
    "                scores += probs[0].detach().cpu().numpy() >= threshold\n",
    "                lu.append(uncertainty[0].item())\n",
    "                ldeg.append(j)\n",
    "                lp.append(probs[0].tolist())\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(\"val/fusion/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            \n",
    "            if prev_alpha is not None:\n",
    "                for fuse, name, fuse_acc in zip(self.fusion_methods, \n",
    "                                                  self.fusion_names, \n",
    "                                                  self.combine_accuracy):\n",
    "                    \n",
    "                    fused_alpha = fuse(prev_alpha[name].to(self.device).to(torch.float), alpha)\n",
    "                    fused_alpha = F.relu(fused_alpha) + 1\n",
    "                    preds = torch.argmax(fused_alpha, dim=1)\n",
    "                    uncertainty = self.num_classes / torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "                    probs = fused_alpha /  torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "                    fuse_acc.update(preds.to('cpu'), labels.argmax(dim=1).to('cpu'))\n",
    "                    \n",
    "                    self.log(\"val/fusion/accuracy/\"+name, fuse_acc, prog_bar=True)\n",
    "            \n",
    "                    prev_alpha[name] = fused_alpha\n",
    "                if batch_idx  % 10== 0:\n",
    "                    fusion_output.append(preds[0].item())\n",
    "                    fusion_scores += probs[0].detach().cpu().numpy() >= threshold\n",
    "                    print (uncertainty[0].item(), fused_alpha[0], )\n",
    "                    fusion_lu.append(uncertainty[0].item())\n",
    "                    fusion_lp.append(probs[0].tolist())\n",
    "                   \n",
    "            else:\n",
    "                prev_alpha={}\n",
    "                for name in self.fusion_names:\n",
    "                    prev_alpha[name] = alpha\n",
    "                    \n",
    "                if batch_idx % 10 == 0:\n",
    "                    fusion_output.append(\"-\")\n",
    "                    fusion_scores += probs[0].detach().cpu().numpy() >= threshold\n",
    "                    #fusion_scores += np.zeros((1, num_classes)) >= threshold\n",
    "                    fusion_lu.append(uncertainty[0].item())\n",
    "                    fusion_lp.append(probs[0].tolist())\n",
    "                   \n",
    "                    \n",
    "        if batch_idx % 10 == 0:\n",
    "            #logits = self(images)\n",
    "            #alpha = F.relu(logits) + 1\n",
    "            #preds = torch.argmax(alpha, dim=1)\n",
    "            #uncertainty = num_classes / alpha.sum(dim=1)\n",
    "            #drichlet_uncertainty = Dirichlet(alpha)\n",
    "            #plt.subplot(1, len(list_images), j+1)\n",
    "            axs[0].imshow(rimgs, vmin=0, vmax=1)  # convert CHW -> HWC\n",
    "            plt.title(labels[0].argmax().to('cpu').item())\n",
    "            axs[0].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(classifications)\n",
    "            axs[1].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[1].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(fusion_output)\n",
    "            axs[2].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[2].axis(\"off\")\n",
    "            \n",
    "            axs[3].plot(ldeg, lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            labels = np.arange(10)[scores[0].astype(bool)]\n",
    "            lp = np.array(lp)[:, labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            labels = labels.tolist()\n",
    "            for i in range(len(labels)):\n",
    "                print (\"plot  lp \", lp[:, i])\n",
    "                axs[3].plot(ldeg, lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[3].set_ylim([0, 1])\n",
    "            axs[3].legend(labels)\n",
    "                \n",
    "            fusion_labels = np.arange(10)[fusion_scores[0].astype(bool)]\n",
    "            fusion_lp = np.array(fusion_lp)[:, fusion_labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            fusion_labels = fusion_labels.tolist()\n",
    "            print (\"fusion_labels \", fusion_labels)\n",
    "            for i in range(len(fusion_labels)):\n",
    "                print (\"plot fusion lp \", fusion_lp[:, i])\n",
    "                axs[4].plot(ldeg, fusion_lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[4].plot(ldeg, fusion_lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            axs[4].set_ylim([0, 1])\n",
    "            axs[4].legend(labels)\n",
    "            axs[4].set_xlim([0, 1])        \n",
    "            \n",
    "            \n",
    "            # log figure\n",
    "            self.logger.experiment.add_figure('sample', fig, global_step=self.global_step)\n",
    "\n",
    "            \n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"test/loss\", loss, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=1, eta_min=1e-4, last_epoch=-1)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        \n",
    "          \n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "        Zoom_MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            #self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "            self.mnist_zoom = Zoom_MNIST(self.data_dir, train=False, transform=self.val_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e5179f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized fusion type :  bayes\n",
      "Initialized fusion type :  dampster\n"
     ]
    }
   ],
   "source": [
    "model = LitMNIST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9d9c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                      | Type                   | Params\n",
      "----------------------------------------------------------------------\n",
      "0  | model                     | ResNet                 | 11.2 M\n",
      "1  | val_accuracy              | Accuracy               | 0     \n",
      "2  | test_0_accuracy           | Accuracy               | 0     \n",
      "3  | test_1_accuracy           | Accuracy               | 0     \n",
      "4  | test_2_accuracy           | Accuracy               | 0     \n",
      "5  | train_cm                  | ConfusionMatrix        | 0     \n",
      "6  | valid_cm                  | ConfusionMatrix        | 0     \n",
      "7  | DS_combine                | DempsterSchaferCombine | 0     \n",
      "8  | mean_combine              | MeanUncertainty        | 0     \n",
      "9  | sum_combine               | SumUncertainty         | 0     \n",
      "10 | bayesian                  | EffectiveProbability   | 0     \n",
      "11 | dampster                  | EffectiveProbability   | 0     \n",
      "12 | conv_1d                   | Sequential             | 210   \n",
      "13 | DS_combine_accuracy       | Accuracy               | 0     \n",
      "14 | mean_combine_accuracy     | Accuracy               | 0     \n",
      "15 | sum_combine_accuracy      | Accuracy               | 0     \n",
      "16 | bayes_combine_accuracy    | Accuracy               | 0     \n",
      "17 | dampster_combine_accuracy | Accuracy               | 0     \n",
      "----------------------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.702    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized fusion type :  bayes\n",
      "Initialized fusion type :  dampster\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f2989938194fb599181943949cd4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized fusion type :  bayes\n",
      "Initialized fusion type :  dampster\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger('./lightning_logs/', name=LOGGER_NAME)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    max_epochs=2,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    #logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    check_val_every_n_epoch=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f126467",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model, 'zoom_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "381a8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fd9e47a9b9454ead019b2b3ceb336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90909091032241 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "0.9090909106302852 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "plot  lp  [0.1        0.81637877 0.82229763]\n",
      "fusion_labels  []\n",
      "0.9090909135551 tensor([1.0288, 1.0288, 1.0288, 1.7409, 1.0288, 1.0288, 1.0288, 1.0288, 1.0288,\n",
      "        1.0288], dtype=torch.float64)\n",
      "0.909090906320032 tensor([1.0935, 1.0935, 1.0935, 1.1583, 1.0935, 1.0935, 1.0935, 1.0935, 1.0935,\n",
      "        1.0935], dtype=torch.float64)\n",
      "plot  lp  [0.74091858 0.74678743 0.62718743]\n",
      "fusion_labels  [3]\n",
      "plot fusion lp  [0.74091858 0.15826532 0.10529685]\n",
      "0.9090909081672833 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "0.909090907551533 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "plot  lp  [0.1        0.79953778 0.77896839]\n",
      "fusion_labels  []\n",
      "0.9090909090909092 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "0.909090906320032 tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000,\n",
      "        1.1000], dtype=torch.float64)\n",
      "plot  lp  [0.1        0.59245443 0.79329664]\n",
      "fusion_labels  []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">          Test metric           </span>┃<span style=\"font-weight: bold\">          DataLoader 0          </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           test/loss            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.18410131335258484       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val/fusion/accuracy/0      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.6531999707221985       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val/fusion/accuracy/1      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9024999737739563       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val/fusion/accuracy/2      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9254999756813049       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> val/fusion/accuracy/DS_combine </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9248999953269958       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   val/fusion/accuracy/bayes    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.6531999707221985       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  val/fusion/accuracy/dampster  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.6531999707221985       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val/fusion/accuracy/mean    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9251499772071838       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val/fusion/accuracy/sum     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9249500036239624       </span>│\n",
       "└────────────────────────────────┴────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         Test metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m         DataLoader 0         \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          test/loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.18410131335258484      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val/fusion/accuracy/0     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.6531999707221985      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val/fusion/accuracy/1     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9024999737739563      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val/fusion/accuracy/2     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9254999756813049      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mval/fusion/accuracy/DS_combine\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9248999953269958      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  val/fusion/accuracy/bayes   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.6531999707221985      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m val/fusion/accuracy/dampster \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.6531999707221985      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val/fusion/accuracy/mean   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9251499772071838      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val/fusion/accuracy/sum    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9249500036239624      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└────────────────────────────────┴────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val/fusion/accuracy/0': 0.6531999707221985,\n",
       "  'val/fusion/accuracy/1': 0.9024999737739563,\n",
       "  'val/fusion/accuracy/DS_combine': 0.9248999953269958,\n",
       "  'val/fusion/accuracy/mean': 0.9251499772071838,\n",
       "  'val/fusion/accuracy/sum': 0.9249500036239624,\n",
       "  'val/fusion/accuracy/bayes': 0.6531999707221985,\n",
       "  'val/fusion/accuracy/dampster': 0.6531999707221985,\n",
       "  'val/fusion/accuracy/2': 0.9254999756813049,\n",
       "  'test/loss': 0.18410131335258484}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.model = torch.load( 'zoom_mnist.pt')\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02dbc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bec9ec4",
   "metadata": {},
   "source": [
    "## 1 D fusion Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91bf6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDLitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.loss = evidence_loss.edl_mse_loss\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomAffine(degrees=0,translate=(0.2,0.2),scale=(0.5,1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.model = resnet18( num_classes=10)\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Have ResNet model take in grayscale rather than RGB\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_0_accuracy = Accuracy()\n",
    "        self.test_1_accuracy = Accuracy()\n",
    "        self.test_2_accuracy = Accuracy()\n",
    "        self.test_accuracy = [self.test_0_accuracy,\n",
    "                             self.test_1_accuracy,\n",
    "                             self.test_2_accuracy]\n",
    "        \n",
    "        self.train_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        self.valid_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        \n",
    "        \n",
    "        self.fc_fusion = torch.nn.Linear(2*self.num_classes, self.num_classes,device=self.device )\n",
    "        \n",
    "        \n",
    "        self.one_d_accuray = Accuracy()\n",
    "        self.two_d_accuracy = Accuracy()\n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def common_epoch_step(self, batch, batch_idx, stage):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(stage+\"/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            \n",
    "            if prev_alpha is not None:\n",
    "\n",
    "                fused_alpha = torch.concat((prev_alpha.to(self.device).to(torch.float),alpha), dim=1)\n",
    "                fused_alpha = self.fc_fusion(fused_alpha)\n",
    "                preds = torch.argmax(fused_alpha, dim=1)\n",
    "                \n",
    "                #self.logger.experiment.add_scalars('Accuracy', \n",
    "                #                                   {'fusion'+name: fuse_acc.compute()},\n",
    "                #                                   self.global_step) \n",
    "            \n",
    "                prev_alpha = fused_alpha\n",
    "                \n",
    "\n",
    "            else:\n",
    "                prev_alpha = alpha\n",
    "            \n",
    "        \n",
    "        \n",
    "                    \n",
    "            \n",
    "        loss = evidence_loss.edl_mse_loss(fused_alpha, labels, self.current_epoch, self.num_classes, 1)\n",
    "        self.one_d_accuray.update(preds, labels.argmax(dim=1))\n",
    "        self.log(stage+\"/one D accuracy\", self.one_d_accuray, prog_bar=True)\n",
    "                \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(stage+\"/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'valid')\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.AdamW(self.fc_fusion.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=1, eta_min=1e-4, last_epoch=-1)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        \n",
    "          \n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        Zoom_MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.mnist_zoom_train = Zoom_MNIST(self.data_dir, train=True, transform=self.val_transform)\n",
    "            self.mnist_zoom_valid = Zoom_MNIST(self.data_dir, train=False, transform=self.val_transform)\n",
    "\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_valid, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54931b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56051aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type            | Params\n",
      "----------------------------------------------------\n",
      "0 | model           | ResNet          | 11.2 M\n",
      "1 | val_accuracy    | Accuracy        | 0     \n",
      "2 | test_0_accuracy | Accuracy        | 0     \n",
      "3 | test_1_accuracy | Accuracy        | 0     \n",
      "4 | test_2_accuracy | Accuracy        | 0     \n",
      "5 | train_cm        | ConfusionMatrix | 0     \n",
      "6 | valid_cm        | ConfusionMatrix | 0     \n",
      "7 | fc_fusion       | Linear          | 210   \n",
      "8 | one_d_accuray   | Accuracy        | 0     \n",
      "9 | two_d_accuracy  | Accuracy        | 0     \n",
      "----------------------------------------------------\n",
      "210       Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.702    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399f6b31d7ec499a8429ef185d6f8867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = OneDLitMNIST()\n",
    "model.model = torch.load('zoom_mnist.pt')\n",
    "model.model.eval()\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "logger = TensorBoardLogger('./lightning_logs/', name=LOGGER_NAME)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    max_epochs=2,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    #logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    check_val_every_n_epoch=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "809bd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DampsterLitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.loss = evidence_loss.edl_mse_loss\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomAffine(degrees=0,translate=(0.2,0.2),scale=(0.5,1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "        self.val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.model = resnet18( num_classes=10)\n",
    "            \n",
    "        # Have ResNet model take in grayscale rather than RGB\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "\n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_0_accuracy = Accuracy()\n",
    "        self.test_1_accuracy = Accuracy()\n",
    "        self.test_2_accuracy = Accuracy()\n",
    "        self.test_accuracy = [self.test_0_accuracy,\n",
    "                             self.test_1_accuracy,\n",
    "                             self.test_2_accuracy]\n",
    "        \n",
    "        self.train_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        self.valid_cm = ConfusionMatrix(num_classes=self.num_classes, normalize='true')\n",
    "        \n",
    "        \n",
    "        #self.fc_fusion = torch.nn.Linear(2*self.num_classes, self.num_classes,device=self.device )\n",
    "        self.DS_combine = uncertain_fusion.DempsterSchaferCombine(self.num_classes)\n",
    "        \n",
    "        self.one_d_accuray = Accuracy()\n",
    "        self.two_d_accuracy = Accuracy()\n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def common_epoch_step(self, batch, batch_idx, stage):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(stage+\"/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            \n",
    "            if prev_alpha is not None:\n",
    "\n",
    "                #fused_alpha = torch.concat((prev_alpha.to(self.device).to(torch.float),alpha), dim=1)\n",
    "                fused_alpha = self.DS_combine(prev_alpha.to(self.device).to(torch.float), alpha)\n",
    "                preds = torch.argmax(fused_alpha, dim=1)\n",
    "                \n",
    "                #self.logger.experiment.add_scalars('Accuracy', \n",
    "                #                                   {'fusion'+name: fuse_acc.compute()},\n",
    "                #                                   self.global_step) \n",
    "            \n",
    "                prev_alpha = fused_alpha\n",
    "                \n",
    "\n",
    "            else:\n",
    "                prev_alpha = alpha\n",
    "            \n",
    "           \n",
    "        loss = evidence_loss.edl_mse_loss(fused_alpha, labels, self.current_epoch, self.num_classes, 1)\n",
    "        self.one_d_accuray.update(preds, labels.argmax(dim=1))\n",
    "        self.log(stage+\"/one D accuracy\", self.one_d_accuray, prog_bar=True)\n",
    "                \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(stage+\"/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.common_epoch_step(batch, batch_idx, 'valid')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        list_images, labels = batch\n",
    "        labels = F.one_hot(labels.to(torch.long), self.num_classes)\n",
    "        \n",
    "        prev_alpha = None\n",
    "        zoom_images = 3\n",
    "        threshold = 0.2\n",
    "        if batch_idx  % 10== 0:\n",
    "            fig = plt.figure(figsize=[6, 5])\n",
    "            fig, axs = plt.subplots(5, gridspec_kw={\"height_ratios\": [5, 1, 1, 10, 10]})\n",
    "            rimgs = np.zeros((28, 28 * zoom_images))\n",
    "            scores = np.zeros((1, num_classes))\n",
    "            fusion_scores = np.zeros((1, num_classes))\n",
    "            classifications = []\n",
    "            fusion_output = []\n",
    "            lu = []\n",
    "            fusion_lu = []\n",
    "            lp = []\n",
    "            fusion_lp = []\n",
    "            ldeg = []\n",
    "            \n",
    "        for j, batch_images in enumerate( list_images):\n",
    "            logits = self(batch_images)\n",
    "            loss = evidence_loss.edl_mse_loss(logits, labels, self.current_epoch, self.num_classes, 5)\n",
    "            #loss = F.nll_loss(logits, labels)\n",
    "            alpha = F.relu(logits) + 1\n",
    "            preds = torch.argmax(alpha, dim=1)\n",
    "            self.test_accuracy[j].update(preds, labels.argmax(dim=1))\n",
    "            uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            if 0 == j:#Find the image with mx uncertainty in the first image only\n",
    "                max_uncertain_args = torch.argmax(uncertainty)\n",
    "            probs = alpha /  torch.sum(alpha, dim=1, keepdim=True)\n",
    "            if batch_idx  % 10 == 0:\n",
    "                \n",
    "                rimgs[:, j*28:(j+1)*28] = batch_images[max_uncertain_args].detach().cpu().numpy().reshape(28,28)\n",
    "                classifications.append(preds[max_uncertain_args].item())\n",
    "                scores += probs[max_uncertain_args].detach().cpu().numpy() >= threshold\n",
    "                lu.append(uncertainty[max_uncertain_args].item())\n",
    "                ldeg.append(j)\n",
    "                lp.append(probs[max_uncertain_args].tolist())\n",
    "            \n",
    "            #self.logger.experiment.add_scalars('Accuracy',\n",
    "            #                                   {'fusion/'+str(j): self.test_accuracy[j].compute()},\n",
    "            #                                   self.global_step) \n",
    "            self.log(\"test/fusion/accuracy/\"+str(j), self.test_accuracy[j], prog_bar=True)\n",
    "            if prev_alpha is not None:\n",
    "                \n",
    "                    \n",
    "                fused_alpha = self.DS_combine(prev_alpha.to(self.device).to(torch.float), alpha)\n",
    "                fused_alpha = F.relu(fused_alpha) + 1\n",
    "                preds = torch.argmax(fused_alpha, dim=1)\n",
    "                uncertainty = self.num_classes / torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "                probs = fused_alpha /  torch.sum(fused_alpha, dim=1, keepdim=True)\n",
    "\n",
    "                prev_alpha = fused_alpha\n",
    "                if batch_idx  % 10== 0:\n",
    "                    fusion_output.append(preds[max_uncertain_args].item())\n",
    "                    fusion_scores += probs[max_uncertain_args].detach().cpu().numpy() >= threshold\n",
    "                    print (uncertainty[max_uncertain_args].item(), fused_alpha[max_uncertain_args], )\n",
    "                    fusion_lu.append(uncertainty[max_uncertain_args].item())\n",
    "                    fusion_lp.append(probs[max_uncertain_args].tolist())\n",
    "                   \n",
    "            else:\n",
    "                prev_alpha = alpha\n",
    "                uncertainty = self.num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "                probs = alpha /  torch.sum(alpha, dim=1, keepdim=True)\n",
    "    \n",
    "                if batch_idx % 10 == 0:\n",
    "                    fusion_output.append(\"-\")\n",
    "                    fusion_scores += probs[max_uncertain_args].detach().cpu().numpy() >= threshold\n",
    "                    #fusion_scores += np.zeros((1, num_classes)) >= threshold\n",
    "                    fusion_lu.append(uncertainty[0].item())\n",
    "                    fusion_lp.append(probs[max_uncertain_args].tolist())\n",
    "                    \n",
    "        self.one_d_accuray.update(preds, labels.argmax(dim=1))\n",
    "        self.log(\"test/fusion/accuracy/one_D_accuracy\", self.one_d_accuray, prog_bar=True)\n",
    "                   \n",
    "                    \n",
    "        if batch_idx % 10 == 0:\n",
    "            #logits = self(images)\n",
    "            #alpha = F.relu(logits) + 1\n",
    "            #preds = torch.argmax(alpha, dim=1)\n",
    "            #uncertainty = num_classes / alpha.sum(dim=1)\n",
    "            #drichlet_uncertainty = Dirichlet(alpha)\n",
    "            #plt.subplot(1, len(list_images), j+1)\n",
    "            axs[0].imshow(rimgs, vmin=0, vmax=1)  # convert CHW -> HWC\n",
    "            plt.title(labels[max_uncertain_args].argmax().to('cpu').item())\n",
    "            axs[0].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(classifications)\n",
    "            axs[1].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[1].axis(\"off\")\n",
    "            \n",
    "            empty_lst = []\n",
    "            empty_lst.append(fusion_output)\n",
    "            axs[2].table(cellText=empty_lst, bbox=[0, 1, 1, 1])\n",
    "            axs[2].axis(\"off\")\n",
    "            \n",
    "            axs[3].plot(ldeg, lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            labels = np.arange(10)[scores[0].astype(bool)]\n",
    "            lp = np.array(lp)[:, labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            labels = labels.tolist()\n",
    "            for i in range(len(labels)):\n",
    "                print (\"plot  lp \", lp[:, i])\n",
    "                axs[3].plot(ldeg, lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[3].set_ylim([0, 1])\n",
    "            axs[3].legend(labels)\n",
    "                \n",
    "            fusion_labels = np.arange(10)[fusion_scores[0].astype(bool)]\n",
    "            fusion_lp = np.array(fusion_lp)[:, fusion_labels]\n",
    "            c = [\"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "            marker = [\"s\", \"^\", \"o\"]*2\n",
    "            fusion_labels = fusion_labels.tolist()\n",
    "            print (\"fusion_labels \", fusion_labels)\n",
    "            for i in range(len(fusion_labels)):\n",
    "                print (\"plot fusion lp \", fusion_lp[:, i])\n",
    "                axs[4].plot(ldeg, fusion_lp[:, i], marker=marker[i], c=c[i])\n",
    "            axs[4].plot(ldeg, fusion_lu, marker=\"<\", c=\"black\")\n",
    "            \n",
    "            axs[4].set_ylim([0, 1])\n",
    "            axs[4].legend(labels)\n",
    "            axs[4].set_xlim([0, 1])        \n",
    "            \n",
    "            \n",
    "            # log figure\n",
    "            self.logger.experiment.add_figure('sample_'+str(batch_idx), fig, global_step=self.global_step)\n",
    "\n",
    "            \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=1, eta_min=1e-4, last_epoch=-1)\n",
    "        #scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        \n",
    "          \n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        Zoom_MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print (\"setup stage \", stage)\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        #if stage == \"fit\" or stage is None:\n",
    "        self.mnist_zoom_train = Zoom_MNIST(self.data_dir, train=True, transform=self.val_transform)\n",
    "        \n",
    "        self.mnist_zoom_valid = Zoom_MNIST(self.data_dir, train=False, transform=self.val_transform)\n",
    "\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_valid, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_zoom_valid, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43854079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model           | ResNet                 | 11.2 M\n",
      "1 | val_accuracy    | Accuracy               | 0     \n",
      "2 | test_0_accuracy | Accuracy               | 0     \n",
      "3 | test_1_accuracy | Accuracy               | 0     \n",
      "4 | test_2_accuracy | Accuracy               | 0     \n",
      "5 | train_cm        | ConfusionMatrix        | 0     \n",
      "6 | valid_cm        | ConfusionMatrix        | 0     \n",
      "7 | DS_combine      | DempsterSchaferCombine | 0     \n",
      "8 | one_d_accuray   | Accuracy               | 0     \n",
      "9 | two_d_accuracy  | Accuracy               | 0     \n",
      "-----------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.701    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup stage  TrainerFn.FITTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8571b7bf7d4375924b76c0ea605420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DampsterLitMNIST()\n",
    "#model.model = torch.load('zoom_mnist.pt')\n",
    "\n",
    "logger = TensorBoardLogger('./lightning_logs/', name=LOGGER_NAME)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    max_epochs=2,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    #logger=CSVLogger(save_dir=\"logs/\"),\n",
    "    check_val_every_n_epoch=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5b779cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup stage  TrainerFn.TESTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dca7fec59624940ad0cda6f988bfbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2721967399120331 tensor([ 2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,\n",
      "         2.0000, 18.7381], device='cuda:0')\n",
      "0.08796845376491547 tensor([ 3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000,\n",
      "         3.0000, 86.6771], device='cuda:0')\n",
      "plot  lp  [0.1        0.66340208 0.73632145]\n",
      "fusion_labels  [9]\n",
      "plot fusion lp  [0.1        0.51004589 0.76248515]\n",
      "0.2080220878124237 tensor([ 2.0000,  2.0000,  2.0000, 30.0718,  2.0000,  2.0000,  2.0000,  2.0000,\n",
      "         2.0000,  2.0000], device='cuda:0')\n",
      "0.06647595018148422 tensor([  3.0000,   3.0000,   3.0000, 123.4303,   3.0000,   3.0000,   3.0000,\n",
      "          3.0000,   3.0000,   3.0000], device='cuda:0')\n",
      "plot  lp  [0.1        0.76360464 0.73244661]\n",
      "fusion_labels  [3]\n",
      "plot fusion lp  [0.1        0.62556022 0.82051492]\n",
      "0.21273541450500488 tensor([ 2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000, 29.0067,\n",
      "         2.0000,  2.0000], device='cuda:0')\n",
      "0.06737329810857773 tensor([  3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,\n",
      "        121.4268,   3.0000,   3.0000], device='cuda:0')\n",
      "plot  lp  [0.1        0.75680107 0.73571098]\n",
      "fusion_labels  [7]\n",
      "plot fusion lp  [0.1        0.61707628 0.81809205]\n",
      "0.16570894420146942 tensor([ 2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000,\n",
      "         2.0000, 42.3468], device='cuda:0')\n",
      "0.056221503764390945 tensor([  3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,   3.0000,\n",
      "          3.0000,   3.0000, 150.8679], device='cuda:0')\n",
      "plot  lp  [0.1        0.82123977 0.70911622]\n",
      "fusion_labels  [9]\n",
      "plot fusion lp  [0.1        0.70172387 0.84820193]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">             Test metric             </span>┃<span style=\"font-weight: bold\">            DataLoader 0             </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/fusion/accuracy/0        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.6639999747276306          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/fusion/accuracy/1        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.9383999705314636          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/fusion/accuracy/2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.9682999849319458          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test/fusion/accuracy/one_D_accuracy </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.9778000116348267          </span>│\n",
       "└─────────────────────────────────────┴─────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m            Test metric            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m           DataLoader 0            \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/fusion/accuracy/0       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.6639999747276306         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/fusion/accuracy/1       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.9383999705314636         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/fusion/accuracy/2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.9682999849319458         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest/fusion/accuracy/one_D_accuracy\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.9778000116348267         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────────────┴─────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/fusion/accuracy/0': 0.6639999747276306,\n",
       "  'test/fusion/accuracy/1': 0.9383999705314636,\n",
       "  'test/fusion/accuracy/2': 0.9682999849319458,\n",
       "  'test/fusion/accuracy/one_D_accuracy': 0.9778000116348267}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f2abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
