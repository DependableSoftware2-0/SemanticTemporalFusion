{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d634cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e9179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "from kornia import image_to_tensor, tensor_to_image\n",
    "from kornia.augmentation import ColorJitter, RandomChannelShuffle, RandomThinPlateSpline\n",
    "from kornia.augmentation import RandomVerticalFlip, RandomHorizontalFlip, Resize, RandomCrop, RandomMotionBlur\n",
    "from kornia.augmentation import RandomEqualize, RandomGaussianBlur, RandomGaussianNoise, RandomSharpness\n",
    "import kornia as K\n",
    "\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kornia.augmentation import Resize\n",
    "\n",
    "from pytransform3d.transform_manager import TransformManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c57ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts,CosineAnnealingLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import vkitti_dataloader\n",
    "import epipolar_geometry\n",
    "import evidence_loss\n",
    "import uncertain_fusion\n",
    "\n",
    "from metrics import IoU, SegmentationMetric\n",
    "from vkitti_dataloader import SequentialImageVirtualKittiDataset\n",
    "#from vkitti_dataloader import SingleImageVirtualKittiDataset \n",
    "\n",
    "from kornia import image_to_tensor, tensor_to_image\n",
    "from kornia.augmentation import ColorJitter, RandomChannelShuffle, RandomThinPlateSpline\n",
    "from kornia.augmentation import RandomVerticalFlip, RandomHorizontalFlip, Resize, RandomCrop, RandomMotionBlur\n",
    "from kornia.augmentation import RandomEqualize, RandomGaussianBlur, RandomGaussianNoise, RandomSharpness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03264d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SingleImageVirtualKittiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root: str, \n",
    "        mode: str = \"train\", \n",
    "        transforms = None):\n",
    "\n",
    "        assert mode in {\"train\", \"valid\", \"test\"}\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.files_directory = root\n",
    "  \n",
    "        self.data_column_names=['scene', 'scenario', 'camera_number', 'frame_number', 'extrinsic']\n",
    "        #if you want a subset of the data\n",
    "        self.subset = ['15-deg-left', '15-deg-right', '30-deg-left', '30-deg-right','clone', 'fog']\n",
    "        self.val_subset = ['morning', 'overcast', 'rain', 'sunset']\n",
    "        \n",
    "        #Filenames extracted as a pandas dataframe\n",
    "        self.filenames = self._read_split()  # read train/valid/test splits\n",
    "        self.mask_colors = pd.read_csv(os.path.join(self.files_directory, \n",
    "                                               'colors.txt'), delimiter=' ')\n",
    "        self.mask_colors['mask_label'] = self.mask_colors.index\n",
    "        #Replacing the follwing\n",
    "        '''\n",
    "        [['Terrain',     0   1],\n",
    "         ['Sky',         1   2],\n",
    "         ['Tree',        2   3],\n",
    "         ['Vegetation'   3   3],\n",
    "         ['Building',    4   4],\n",
    "         ['Road',        5   5],\n",
    "         ['GuardRail',   6   0],\n",
    "         ['TrafficSign', 7   0],\n",
    "         ['TrafficLight',8   0],\n",
    "         ['Pole',        9   0],\n",
    "         ['Misc', ,      10  0],\n",
    "         ['Truck',       11  6],\n",
    "         ['Car', ,       12  6],\n",
    "         ['Van',         13  6],\n",
    "         ['Undefined',   14  0]]\n",
    "        '''\n",
    "        current_labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "        replace_labels = [1,2,3,3,4,5,0,0,0,0, 0, 6, 6, 6, 0]\n",
    "        self.mask_colors['mask_label'] =self.mask_colors['mask_label'].replace(current_labels, \n",
    "                                                                               replace_labels)\n",
    "        print (\"Total classes \", len(self.mask_colors['mask_label'].unique()))\n",
    "        print (\"Total classes \", (self.mask_colors['mask_label'].unique()))\n",
    "        #Final classes are\n",
    "        self.mask_colors = self.mask_colors.values.tolist()\n",
    "        self.label_names = ['misc', 'Terrain', 'Sky', 'Tree', 'Building', 'Road', 'Vehicle']\n",
    "      \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "\n",
    "        _, scene, scenario, camera, frame_number, x = self.filenames[idx]\n",
    "        data_filename = scene+'_'+scenario+'_'+camera+'_'+str(frame_number).zfill(5)+'.h5'\n",
    "        data_filename = os.path.join(self.files_directory, data_filename)\n",
    "        sample = {}\n",
    "        #Reading image as numpy\n",
    "        with h5py.File(data_filename, 'r') as data: \n",
    "            sample['image'] = np.asarray(Image.open(io.BytesIO(np.array(data['image']))))\n",
    "            sample['mask'] = np.asarray(Image.open(io.BytesIO(np.array(data['mask']))))\n",
    "            #sample['depth'] = np.asarray(Image.open(io.BytesIO(np.array(data['depth']))))\n",
    "            transformation_matrices=np.array(data['extrinsic'])\n",
    "            #Was geting a user warning that array is not writeable and pytroch needs writeable\n",
    "            sample['image'] = np.copy(sample['image'])\n",
    "            sample['mask'] = np.copy(sample['mask'])\n",
    "            #sample['depth'] = np.copy(sample['depth'])\n",
    "            \n",
    "\n",
    "        sample['mask'] = self._preprocess_mask(sample['mask'])\n",
    "        \n",
    "        #Applies transformation and converts to tensor\n",
    "        if self.transforms is not None:            \n",
    "            transformed = self.transforms(image=sample['image'], \n",
    "                                          mask=sample['mask'],\n",
    "                                          depth=None)\n",
    "\n",
    "            sample['image'] = transformed['image']\n",
    "            sample['mask'] = transformed['mask'].long()\n",
    "        \n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def _preprocess_mask(self, mask: np.ndarray) -> np.ndarray:\n",
    "        ''' \n",
    "        Convert RGB mask to single channel mask based on the color value\n",
    "        provided in color.txt file \n",
    "        \n",
    "        Parameters:\n",
    "            mask: Numpy array mask of shape [height, width, 3]\n",
    "            out: Numpy array of shape [height, width]\n",
    "        '''\n",
    "        preprocessed_mask = np.zeros(mask.shape[:2])\n",
    "        for index, row in enumerate(self.mask_colors):\n",
    "            # The columns of  mask_color dataframe is ['Terrain', r, g, b, mask_label]\n",
    "            idx = np.all(mask == (row[1], row[2], row[3]), axis=-1) #\n",
    "            preprocessed_mask[idx] = row[4]\n",
    "\n",
    "        return preprocessed_mask\n",
    "\n",
    "    def _read_split(self) -> list:\n",
    "        ''' \n",
    "        Parses the virual kitti dataset and converts to a pandas dataframe\n",
    "        \n",
    "        Parameters:\n",
    "            out: A list\n",
    "        '''\n",
    "\n",
    "        filenames = pd.read_csv(self.files_directory+'/virtual_kiti_file_naming.csv')\n",
    "                \n",
    "        if self.mode == \"train\":  # 90% for train\n",
    "            # Creating a dataframe with 50%\n",
    "            #filenames = filenames.sample(frac = 0.6, random_state=55)\n",
    "            filenames = filenames[filenames['scenario'].isin(self.subset)]\n",
    "        elif self.mode == \"valid\":  # 10% for validation\n",
    "            #sampling the same files with the random_state and droping them\n",
    "            #train_filenames = filenames.sample(frac = 0.6, random_state=55)\n",
    "            #filenames = filenames.drop(train_filenames.index)\n",
    "            filenames = filenames[filenames['scenario'].isin(self.val_subset)]\n",
    "            \n",
    "        return filenames.values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "269c0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 256\n",
    "old_k = np.array([[725.0087, 0, 620.5],\n",
    "                   [0, 725.0087, 187],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "K = np.array([[725.0087*(IMG_SIZE/1242), 0, IMG_SIZE/2],\n",
    "                   [0, 725.0087*(IMG_SIZE/375), IMG_SIZE/2],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Kinv= np.linalg.inv(K)\n",
    "\n",
    "class VirtualKittiModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "        arch='Unet', \n",
    "        encoder_name='resnet18', \n",
    "        in_channels=3, \n",
    "        out_classes=7,\n",
    "        dataset_path=None,\n",
    "\t\t**kwargs\n",
    "\t):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch, \n",
    "            encoder_name=encoder_name, \n",
    "            in_channels=in_channels, \n",
    "            classes=out_classes, \n",
    "            #**kwargs\n",
    "        )\n",
    "\n",
    "        self.epipolar_propagation = epipolar_geometry.EpipolarPropagation(K, \n",
    "                                   Kinv, \n",
    "                                   IMG_SIZE, \n",
    "                                   IMG_SIZE, \n",
    "                                   fill_empty_with_ones=True)\n",
    "        self.epipolar_propagation.cuda()\n",
    "\n",
    "        self.kornia_pre_transform = vkitti_dataloader.Preprocess() #per image convert to tensor\n",
    "        self.transform = torch.nn.Sequential(\n",
    "                RandomHorizontalFlip(p=0.30),\n",
    "                RandomChannelShuffle(p=0.10),\n",
    "                RandomThinPlateSpline(p=0.10),\n",
    "                RandomEqualize(p=0.2),\n",
    "                RandomGaussianBlur((3, 3), (0.1, 2.0), p=0.2),\n",
    "                RandomGaussianNoise(mean=0., std=1., p=0.2),\n",
    "                RandomSharpness(0.5, p=0.2)\n",
    "            )\n",
    "     \n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=True)\n",
    "        self.evidence_loss_fn = evidence_loss.edl_mse_loss\n",
    "        self.n_classes = out_classes\n",
    "        \n",
    "        self.val_0_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        self.val_1_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        self.ds_fusion_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        self.sum_fusion_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        self.mean_fusion_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        self.train_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "\n",
    "        self.val_0_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "        self.val_1_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "        self.ds_fusion_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "        self.sum_fusion_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "        self.mean_fusion_seg_metric = SegmentationMetric(self.n_classes).cuda()        \n",
    "        \n",
    "        #self.train_cm = torchmetrics.ConfusionMatrix(num_classes=self.n_classes, normalize='true')\n",
    "        #kself.valid_cm = torchmetrics.ConfusionMatrix(num_classes=self.n_classes)\n",
    "        \n",
    "        self.DS_combine = uncertain_fusion.DempsterSchaferCombine(self.n_classes)\n",
    "        self.mean_combine = uncertain_fusion.MeanUncertainty(self.n_classes)\n",
    "        self.sum_combine = uncertain_fusion.SumUncertainty(self.n_classes)        \n",
    "\n",
    "        self.fusion_methods = [self.DS_combine, self.mean_combine, self.sum_combine]#,self.bayesian, ]\n",
    "        self.fusion_names = ['DS_combine', 'mean', 'sum']#'bayes',\n",
    "        self.fusion_iou = [self.ds_fusion_iou, \n",
    "                                self.mean_fusion_iou,\n",
    "                                self.sum_fusion_iou,\n",
    "                                #self.bayes_fusion_iou,\n",
    "                                #self.dampster_fusion_accuracy\n",
    "                               ]\n",
    "        self.fusion_seg_metric = [ self.ds_fusion_seg_metric, \n",
    "                                #   self.bayes_fusion_seg_metric,\n",
    "                                   self.mean_fusion_seg_metric ,\n",
    "                                   self.sum_fusion_seg_metric,\n",
    "                                 ]\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        \n",
    "\n",
    "    def forward(self, image):\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print(\"train_step %d torch.cuda.memory_allocated: %fGB\"%(batch_idx, torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "        print(\"train_step %d torch.cuda.memory_reserved: %fGB\"%(batch_idx, torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "        print(\"train_step %d torch.cuda.max_memory_reserved: %fGB\"%(batch_idx, torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "        image = batch[\"image\"]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "        bs, num_channels, height, width = image.size()\n",
    "\n",
    "        # Check that image dimensions are divisible by 32, \n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[\"mask\"]\n",
    "        # Shape of the mask should be [batch_size, height, width]\n",
    "        assert mask.ndim == 3\n",
    "        \n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 255.0 and mask.min() >= 0\n",
    "      \n",
    "        \n",
    "        logits_mask = self.forward(image)\n",
    "        #clamping highest dirchlet value \n",
    "        logits_mask = torch.clamp(logits_mask, max=50)\n",
    "\n",
    "\n",
    "        ## DICE LOSS CALCULATION\n",
    "        dice_loss = self.dice_loss_fn(logits_mask, mask)\n",
    "        print (\"dice_loss \", dice_loss.item())\n",
    "\n",
    "        ## EVIDENTIAL LOSS CALCULATION\n",
    "        #unroll the mask to single tensor \n",
    "        # [batch_size, height, width] -> [batch_size*height*width]\n",
    "        mask = torch.ravel(mask)\n",
    "        # [batch_size*height*width] -> [batch_size*height*width, n_classes] \n",
    "        mask = F.one_hot(mask.to(torch.long), self.n_classes)\n",
    "        # [batch_size, n_classes, height, width] -> [batch_size,n_classes, height*width]\n",
    "        #logits_mask = logits_mask.view(bs, self.n_classes, -1) \n",
    "        # [batch_size,n_classes, height*width] -> [batch_size, height*width, n_classes]\n",
    "        logits_mask = logits_mask.permute(0,2,3,1)\n",
    "        # [batch_size, height*width, n_classes] -> [batch_size*height*width, n_classes]\n",
    "        logits_mask = logits_mask.reshape(-1, self.n_classes)\n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.evidence_loss_fn(logits_mask, mask, self.current_epoch, self.n_classes, 5)\n",
    "\t\t   \n",
    "       \n",
    "        logits_mask = torch.relu(logits_mask) + 1\n",
    "        pred_mask = logits_mask.argmax(dim=1, keepdim=True)\n",
    "        mask = mask.argmax(dim=1, keepdim=True)\n",
    "      \n",
    "        #loging confusion matrix and segmentation metrics \n",
    "        #self.train_cm(pred_mask, mask)\n",
    "            \n",
    "        #Changing back to original dimension for metrics calculation\n",
    "        pred_mask = pred_mask.reshape(bs, 1, height, width )\n",
    "        mask = mask.reshape(bs, 1, height, width)\n",
    "        \n",
    "        #self.train_seg_metric.addBatch(pred_mask.long(), mask.long())\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), \n",
    "                                               mask.long(), \n",
    "                                               mode=\"multiclass\", \n",
    "                                               num_classes=self.n_classes)\n",
    "        print (\"=======================\")\n",
    "        #del image, mask, logits_mask, pred_mask\n",
    "        if (batch_idx == 2):\n",
    "            assert True\n",
    "        return {\n",
    "            \"loss\": loss, #ToDo restur combined ones based on analysis\n",
    "            \"dice_loss\": dice_loss.item(),\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "        \n",
    "    def on_after_batch_transfer(self, batch, dataloader_idx):\n",
    "        if self.trainer.training:\n",
    "            image = batch[\"image\"]\n",
    "            mask = batch[\"mask\"]\n",
    "            image = self.transform(image)  # => we perform GPU/Batched data augmentation\n",
    "            return {'image':image , 'mask':mask}\n",
    "        else:\n",
    "            return batch\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image \n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset \n",
    "        # with \"empty\" images (images without target class) a large gap could be observed. \n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        \n",
    "       \n",
    "        # aggregate step metics\n",
    "        loss = [x[\"loss\"].item() for x in outputs]\n",
    "        loss = sum(loss)/len(loss)\n",
    "        dice_loss = [x[\"dice_loss\"].item() for x in outputs]\n",
    "        dice_loss = sum(dice_loss)/len(dice_loss)\n",
    "        \n",
    "        metrics = {\n",
    "            f\"train/per_image_iou\": per_image_iou,\n",
    "            f\"train/dataset_iou\": dataset_iou,\n",
    "            f\"train/evidential_loss\": loss,\n",
    "            f\"train/dice_loss\": dice_loss,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        # turn confusion matrix into a figure (Tensor cannot be logged as a scalar)\n",
    "        #fig, ax = plt.subplots(figsize=(20,20))\n",
    "        #disp = ConfusionMatrixDisplay(confusion_matrix=self.train_cm.compute().cpu().numpy(),\n",
    "        #                              display_labels=self.label_names)\n",
    "        #disp.plot(ax=ax)\n",
    "        # log figure\n",
    "        #self.logger.experiment.add_figure('train/confmat', fig, global_step=self.global_step)\n",
    "        \n",
    "        #np.save(CM_FILE_NAME, self.train_cm.compute().cpu().numpy())\n",
    "        #self.log(\"FrequencyIoU/train\",\n",
    "        #     self.train_seg_metric.Frequency_Weighted_Intersection_over_Union(), prog_bar=False)\n",
    "\n",
    "        #self.train_seg_metric.reset()    \n",
    "        #self.train_cm.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "\n",
    "        print(\"validation_steptorch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "        print(\"validation_steptorch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "        print(\"validation_steptorch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "\t    # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert batch[\"image0\"].ndim == 4\n",
    "        \n",
    "        bs, num_channels, height, width = batch[\"image0\"].size()\n",
    "\n",
    "        # Check that image dimensions are divisible by 32, \n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        assert height % 32 == 0 and width % 32 == 0\n",
    "\n",
    "        batch[\"mask0\"] = batch[\"mask0\"].unsqueeze(dim=1)\n",
    "        batch[\"mask1\"] = batch[\"mask1\"].unsqueeze(dim=1)\n",
    "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
    "        # for binary segmentation num_classes = 1\n",
    "        assert batch[\"mask0\"].ndim == 4\n",
    "        assert batch[\"mask1\"].ndim == 4\n",
    "        \n",
    "    \n",
    "        logits_mask0 = self.model(batch[\"image0\"])\n",
    "        logits_mask0= F.relu(logits_mask0) + 1  #ToDO shoudl we do relu and propagate or just propagate\n",
    "        \n",
    "        propagate_mask0 = self.epipolar_propagation(logits_mask0, \n",
    "                                                     batch['depth0']/100,\n",
    "                                                     batch['translation_0_to_1_camera_frame'],\n",
    "                                                     batch['rotation_0_to_1_camera_frame'])\n",
    "        \n",
    "        logits_mask1 = self.model(batch[\"image1\"])\n",
    "        logits_mask1 = F.relu(logits_mask1) + 1\n",
    "        \n",
    "        self.val_0_iou.update(logits_mask0.argmax( dim=1, keepdim=True),batch[\"mask0\"])\n",
    "        self.log(\"val_iou/0\", self.val_0_iou, prog_bar=True)\n",
    "        #print (\"shared \", batch[\"mask0\"].device, logits_mask0.device, self.val_0_seg_metric.confusionMatrix.device )\n",
    "        self.val_0_seg_metric.addBatch(logits_mask0.argmax( dim=1, keepdim=True),batch[\"mask0\"])\n",
    "        self.val_1_iou.update(logits_mask1.argmax( dim=1, keepdim=True),batch[\"mask1\"])\n",
    "        self.log(\"val_iou/1\", self.val_1_iou, prog_bar=True)\n",
    "        self.val_1_seg_metric.addBatch(logits_mask1.argmax( dim=1, keepdim=True),batch[\"mask1\"])\n",
    "        \n",
    "        for fusion, name, iou, seg_metric in zip(self.fusion_methods, \n",
    "                                                 self.fusion_names, \n",
    "                                                 self.fusion_iou,\n",
    "                                                 self.fusion_seg_metric):\n",
    "         \n",
    "            fusion_out = fusion(propagate_mask0, logits_mask1)\n",
    "            fusion_out = fusion_out.to(self.device)\n",
    "            \n",
    "            iou.update(fusion_out.argmax( dim=1, keepdim=True), batch[\"mask1\"])\n",
    "            seg_metric.addBatch(fusion_out.argmax( dim=1, keepdim=True), batch[\"mask1\"])\n",
    "            self.log(\"val_iou/\"+name+\"_fusion\", iou, prog_bar=True)\n",
    "\n",
    "        self.log('val_1/Max val 1', torch.max(logits_mask1), prog_bar=False)\n",
    "        self.log('val_1/Min val 1', torch.min(logits_mask1), prog_bar=False)\n",
    "        self.log('val_1/val 1', torch.mean(logits_mask1), prog_bar=False)\n",
    "            \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log(\"PixelAccuracy/val_0\", \n",
    "                             self.val_0_seg_metric.pixelAccuracy(), prog_bar=False)\n",
    "        self.log(\"MeanIoU/val_0\", \n",
    "             self.val_0_seg_metric.meanIntersectionOverUnion(), prog_bar=False)\n",
    "        self.log(\"FrequencyIoU/val_0\",\n",
    "             self.val_0_seg_metric.Frequency_Weighted_Intersection_over_Union(), prog_bar=False)\n",
    "        self.log(\"PixelAccuracy/val_1\", \n",
    "             self.val_1_seg_metric.pixelAccuracy(), prog_bar=False)\n",
    "        self.log(\"MeanIoU/val_1\", \n",
    "             self.val_1_seg_metric.meanIntersectionOverUnion(), prog_bar=False)\n",
    "        self.log(\"FrequencyIoU/val_1\",\n",
    "             self.val_0_seg_metric.Frequency_Weighted_Intersection_over_Union(), prog_bar=False)\n",
    "        self.val_0_seg_metric.reset()\n",
    "        self.val_1_seg_metric.reset()\n",
    "\n",
    "        for seg_metric, fusion_name in zip(self.fusion_seg_metric, self.fusion_names):\n",
    "            self.log(\"PixelAccuracy/\"+fusion_name, \n",
    "                 seg_metric.pixelAccuracy(), prog_bar=False)\n",
    "            self.log(\"MeanIoU/\"+fusion_name, \n",
    "                 seg_metric.meanIntersectionOverUnion(), prog_bar=False)\n",
    "            self.log(\"FrequencyIoU/\"+fusion_name,\n",
    "                 seg_metric.Frequency_Weighted_Intersection_over_Union(), prog_bar=False)\n",
    "\n",
    "            seg_metric.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=1e-5, amsgrad=True)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "   \n",
    "    def train_dataloader(self):\n",
    "        dataset = SingleImageVirtualKittiDataset(self.dataset_path, \"train\", transforms=self.kornia_pre_transform)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=32)\n",
    "                            #persistent_workers=True, pin_memory=True)\n",
    "        self.label_names = dataset.label_names\n",
    "        print ('Training dataset length : ', len(dataset) )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = SequentialImageVirtualKittiDataset(self.dataset_path, \"valid\", transforms=self.kornia_pre_transform)\n",
    "        loader = DataLoader(dataset, batch_size=10, shuffle=False, num_workers=10)\n",
    "        self.label_names = dataset.label_names\n",
    "        print ('Vaidation dataset length : ', len(dataset))\n",
    "        return loader\n",
    "        \n",
    "#====================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a83d75cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deebuls/miniconda3/envs/tpytorch/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (IoU). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.000023GB\n",
      "torch.cuda.memory_reserved: 0.011719GB\n",
      "torch.cuda.max_memory_reserved: 4.896484GB\n",
      "cuda\n",
      "torch.cuda.memory_allocated: 0.001736GB\n",
      "torch.cuda.memory_reserved: 0.011719GB\n",
      "torch.cuda.max_memory_reserved: 4.896484GB\n",
      "torch.cuda.memory_allocated: 0.001736GB\n",
      "torch.cuda.memory_reserved: 0.011719GB\n",
      "torch.cuda.max_memory_reserved: 4.896484GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                   | Type                   | Params\n",
      "-------------------------------------------------------------------\n",
      "0  | model                  | Unet                   | 14.3 M\n",
      "1  | epipolar_propagation   | EpipolarPropagation    | 0     \n",
      "2  | kornia_pre_transform   | Preprocess             | 0     \n",
      "3  | transform              | Sequential             | 0     \n",
      "4  | dice_loss_fn           | DiceLoss               | 0     \n",
      "5  | val_0_iou              | IoU                    | 0     \n",
      "6  | val_1_iou              | IoU                    | 0     \n",
      "7  | ds_fusion_iou          | IoU                    | 0     \n",
      "8  | sum_fusion_iou         | IoU                    | 0     \n",
      "9  | mean_fusion_iou        | IoU                    | 0     \n",
      "10 | train_seg_metric       | SegmentationMetric     | 0     \n",
      "11 | val_0_seg_metric       | SegmentationMetric     | 0     \n",
      "12 | val_1_seg_metric       | SegmentationMetric     | 0     \n",
      "13 | ds_fusion_seg_metric   | SegmentationMetric     | 0     \n",
      "14 | sum_fusion_seg_metric  | SegmentationMetric     | 0     \n",
      "15 | mean_fusion_seg_metric | SegmentationMetric     | 0     \n",
      "16 | DS_combine             | DempsterSchaferCombine | 0     \n",
      "17 | mean_combine           | MeanUncertainty        | 0     \n",
      "18 | sum_combine            | SumUncertainty         | 0     \n",
      "-------------------------------------------------------------------\n",
      "14.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 M    Total params\n",
      "57.316    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes  7\n",
      "Total classes  [1 2 3 4 5 0 6]\n",
      "Training dataset length :  25512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deebuls/miniconda3/envs/tpytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/deebuls/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:479: UserWarning: You requested to overfit but enabled training dataloader shuffling. We are turning off the training dataloader shuffling for you.\n",
      "  rank_zero_warn(\n",
      "/home/deebuls/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1933: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54a72311c904e26bc5625557983e46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 0 torch.cuda.memory_allocated: 0.095438GB\n",
      "train_step 0 torch.cuda.memory_reserved: 0.177734GB\n",
      "train_step 0 torch.cuda.max_memory_reserved: 4.896484GB\n",
      "dice_loss  0.8744771480560303\n",
      "=======================\n",
      "train_step 1 torch.cuda.memory_allocated: 0.313748GB\n",
      "train_step 1 torch.cuda.memory_reserved: 4.371094GB\n",
      "train_step 1 torch.cuda.max_memory_reserved: 4.896484GB\n",
      "dice_loss  0.8469793796539307\n",
      "=======================\n",
      "train_step 2 torch.cuda.memory_allocated: 0.313749GB\n",
      "train_step 2 torch.cuda.memory_reserved: 4.376953GB\n",
      "train_step 2 torch.cuda.max_memory_reserved: 4.896484GB\n",
      "dice_loss  0.8234192728996277\n",
      "=======================\n",
      "train_step 3 torch.cuda.memory_allocated: 0.313749GB\n",
      "train_step 3 torch.cuda.memory_reserved: 4.376953GB\n",
      "train_step 3 torch.cuda.max_memory_reserved: 4.896484GB\n",
      "dice_loss  0.7990767955780029\n",
      "=======================\n",
      "train_step 4 torch.cuda.memory_allocated: 0.313750GB\n",
      "train_step 4 torch.cuda.memory_reserved: 4.376953GB\n",
      "train_step 4 torch.cuda.max_memory_reserved: 4.896484GB\n",
      "dice_loss  0.7724806666374207\n",
      "=======================\n",
      "train_step 5 torch.cuda.memory_allocated: 0.313750GB\n",
      "train_step 5 torch.cuda.memory_reserved: 4.376953GB\n",
      "train_step 5 torch.cuda.max_memory_reserved: 4.896484GB\n",
      "dice_loss  0.8300449252128601\n",
      "=======================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.cuda.memory_reserved: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mGB\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_reserved(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.cuda.max_memory_reserved: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mGB\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvkitti_model\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    807\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    809\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    810\u001b[0m )\n\u001b[0;32m--> 811\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1236\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:205\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:282\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m epoch_end_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_prepare_outputs_training_epoch_end(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs,\n\u001b[1;32m    277\u001b[0m     lightning_module\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    278\u001b[0m     num_optimizers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers),\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# run lightning module hook training_epoch_end\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# refresh the result for custom logging at the epoch level\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m epoch_end_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_end_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch_end_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_epoch_end` expects a return of None. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHINT: remove the return statement in `training_epoch_end`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tpytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1592\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1595\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mVirtualKittiModel.training_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    215\u001b[0m loss \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m    216\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(loss)\n\u001b[0;32m--> 217\u001b[0m dice_loss \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdice_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m    218\u001b[0m dice_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(dice_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dice_loss)\n\u001b[1;32m    220\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/per_image_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m: per_image_iou,\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/dataset_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_iou,\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/evidential_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/dice_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: dice_loss,\n\u001b[1;32m    225\u001b[0m }\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    215\u001b[0m loss \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m    216\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(loss)\n\u001b[0;32m--> 217\u001b[0m dice_loss \u001b[38;5;241m=\u001b[39m [\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdice_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m    218\u001b[0m dice_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(dice_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dice_loss)\n\u001b[1;32m    220\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/per_image_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m: per_image_iou,\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/dataset_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_iou,\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/evidential_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/dice_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: dice_loss,\n\u001b[1;32m    225\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor,LearningRateMonitor,TQDMProgressBar\n",
    "import torch\n",
    "\n",
    "#dataset_path = '/scratch/dnair2m/virtual_kitti_h5/'\n",
    "dataset_path = '/home/deebuls/Documents/phd/dataset/virtual_kitti_h5/'\n",
    "\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "vkitti_model = VirtualKittiModel(\"Unet\", \"resnet18\", in_channels=3, out_classes=7, dataset_path=dataset_path)\n",
    "\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu', \n",
    "    devices=1,\n",
    "    max_epochs=2,\n",
    "    callbacks=[LearningRateMonitor(logging_interval=\"step\"), \n",
    "               TQDMProgressBar(refresh_rate=1000)],\n",
    "    check_val_every_n_epoch=10,\n",
    "    overfit_batches=6\n",
    "    #resume_from_checkpoint=\"/home/dnair2m/multi-view-fusion-initial/lightning_logs/version_481766/checkpoints/epoch=19-step=15960.ckpt\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "trainer.fit(\n",
    "    vkitti_model\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f741998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9891a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SequenceVkitiModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_path, dataset_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.dataset_path = dataset_path\n",
    "        self.model_path = model_path\n",
    "        self.model = torch.load(model_path)\n",
    "        #Freezing the network\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # preprocessing parameteres for image\n",
    "        self.n_classes = 7\n",
    "        self.dataset_path = dataset_path\n",
    "                        \n",
    "        self.loss_fn = evidence_loss.edl_mse_loss\n",
    "        \n",
    "        self.epipolar_propagation = epipolar_geometry.EpipolarPropagation(K, \n",
    "                                   Kinv, \n",
    "                                   IMG_SIZE, \n",
    "                                   IMG_SIZE, \n",
    "                                   fill_empty_with_ones=True)\n",
    "        self.epipolar_propagation.cuda()\n",
    "\n",
    "        self.kornia_pre_transform = vkitti_dataloader.Preprocess() #per image convert to tensor\n",
    "        \n",
    "        #self.conv_1d = torch.nn.Conv2d(in_channels=2*self.n_classes, out_channels=self.n_classes, kernel_size=1)\n",
    "        self.val_0_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        self.val_1_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        self.OneD_fusion_iou = IoU(n_classes=self.n_classes, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        self.val_0_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "        self.val_1_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "        self.OneD_fusion_seg_metric = SegmentationMetric(self.n_classes).cuda()\n",
    "       \n",
    "        self.confusion_matrix = torchmetrics.ConfusionMatrix(num_classes=self.n_classes, normalize='true')\n",
    "       \n",
    "        \n",
    "        self.conv_1d = torch.nn.Sequential(\n",
    "                          torch.nn.Conv2d(in_channels=2*self.n_classes, \n",
    "                                       out_channels=self.n_classes, \n",
    "                                       kernel_size=1, \n",
    "                                       device=self.device),\n",
    "                         #torch.nn.Upsample(size=(256,256), mode = 'nearest') #Make the image size auto\n",
    "                        )\n",
    "\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        #Freezing the network\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        \n",
    "\n",
    "        logits_mask0 = self.model(batch[\"image0\"])\n",
    "        propagate_mask0 = self.epipolar_propagation(logits_mask0, \n",
    "                                                     batch['depth0']/100,\n",
    "                                                     batch['translation_0_to_1_camera_frame'],\n",
    "                                                     batch['rotation_0_to_1_camera_frame'])\n",
    "        \n",
    "        logits_mask1 = self.model(batch[\"image1\"].to(self.device))\n",
    "      \n",
    "        fused_mask = torch.concat((propagate_mask0,logits_mask1), dim=1)\n",
    "        fused_mask = self.conv_1d(fused_mask)\n",
    "\n",
    "        logits_mask0 = F.relu(logits_mask0) + 1\n",
    "        logits_mask1 = F.relu(logits_mask1) + 1\n",
    "        #fused_mask = F.relu(fused_mask) + 1\n",
    "        \n",
    "        return logits_mask0, propagate_mask0, logits_mask1, fused_mask\n",
    "    \n",
    "   \n",
    "        \n",
    "        \n",
    "    def shared_step(self, batch, stage):\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert batch[\"image0\"].ndim == 4\n",
    "        \n",
    "        bs, num_channels, height, width = batch[\"image0\"].size()\n",
    "\n",
    "        # Check that image dimensions are divisible by 32, \n",
    "        assert height % 32 == 0 and width % 32 == 0\n",
    "\n",
    "        batch[\"mask0\"] = batch[\"mask0\"].unsqueeze(dim=1)\n",
    "        batch[\"mask1\"] = batch[\"mask1\"].unsqueeze(dim=1)\n",
    "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
    "        # for binary segmentation num_classes = 1\n",
    "        assert batch[\"mask0\"].ndim == 4\n",
    "        assert batch[\"mask1\"].ndim == 4\n",
    "        \n",
    "        logits_mask0, propagate_mask0, logits_mask1, fused_mask = self.forward(batch)\n",
    "        \n",
    "        fused_mask = fused_mask.permute(0,2,3,1)\n",
    "        # [batch_size, height*width, n_classes] -> [batch_size*height*width, n_classes]\n",
    "        fused_mask = fused_mask.reshape(-1, self.n_classes)\n",
    "        mask = torch.ravel(batch[\"mask1\"])\n",
    "        # [batch_size*height*width] -> [batch_size*height*width, n_classes] \n",
    "        mask = F.one_hot(mask.to(torch.long), self.n_classes)\n",
    "        #loss = self.loss_fn(fused_mask, mask, self.current_epoch, self.n_classes, 5)\n",
    "        loss = F.cross_entropy(fused_mask, mask.argmax(dim=1).to(torch.long))\n",
    "        self.log(f\"{stage}/evidential_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        fused_mask = F.relu(fused_mask) + 1\n",
    "        #confusion matrix calculation \n",
    "        self.confusion_matrix(fused_mask.argmax( dim=1, keepdim=True), mask.argmax( dim=1, keepdim=True))\n",
    "\n",
    "        #Getting back to shape\n",
    "        fused_mask = fused_mask.reshape(bs,  height, width, self.n_classes)\n",
    "        fused_mask = fused_mask.permute(0,3,1,2)\n",
    "        \n",
    "        #Logging\n",
    "        self.val_0_iou.update(logits_mask0.argmax( dim=1, keepdim=True), batch[\"mask0\"])\n",
    "        self.val_1_iou.update(logits_mask1.argmax( dim=1, keepdim=True), batch[\"mask1\"])\n",
    "        self.OneD_fusion_iou.update(fused_mask.argmax( dim=1, keepdim=True), batch[\"mask1\"])\n",
    "        self.log(f\"iou/{stage}/0_iou\", self.val_0_iou.compute(), prog_bar=False)\n",
    "        self.log(f\"iou/{stage}/1_iou\", self.val_1_iou.compute(), prog_bar=True)\n",
    "        self.log(f\"iou/{stage}/OneD_fusion_iou\", self.OneD_fusion_iou.compute(), prog_bar=True)\n",
    "        \n",
    "        self.val_0_seg_metric.addBatch(logits_mask0.argmax( dim=1, keepdim=True), batch[\"mask0\"])\n",
    "        self.val_1_seg_metric.addBatch(logits_mask1.argmax( dim=1, keepdim=True), batch[\"mask1\"])\n",
    "        self.OneD_fusion_seg_metric.addBatch(fused_mask.argmax( dim=1, keepdim=True), batch[\"mask1\"])\n",
    "\n",
    "        \n",
    "        return loss\n",
    "      \n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        self.log(f\"iou/{stage}/0_iou\", self.val_0_iou.compute(), prog_bar=False)\n",
    "        self.log(f\"iou/{stage}/1_iou\", self.val_1_iou.compute(), prog_bar=True)\n",
    "        self.log(f\"iou/{stage}/OneD_fusion_iou\", self.OneD_fusion_iou.compute(), prog_bar=True)\n",
    "        self.log(\"FrequencyIoU/\"+stage+\"/0\", \n",
    "                         self.val_0_seg_metric.Frequency_Weighted_Intersection_over_Union(), prog_bar=False)\n",
    "        self.log(\"FrequencyIoU/\"+stage+\"/1\", \n",
    "                         self.val_1_seg_metric.Frequency_Weighted_Intersection_over_Union(), prog_bar=True)\n",
    "        self.log(\"FrequencyIoU/\"+stage+\"/OneD_fusion\", \n",
    "                         self.OneD_fusion_seg_metric.Frequency_Weighted_Intersection_over_Union(), prog_bar=True)\n",
    "        self.val_0_seg_metric.reset()\n",
    "        self.val_1_seg_metric.reset()\n",
    "        self.OneD_fusion_seg_metric.reset()\n",
    "        self.val_0_iou.reset()\n",
    "        self.val_1_iou.reset()\n",
    "        self.OneD_fusion_iou.reset()\n",
    "\n",
    "\n",
    "        # turn confusion matrix into a figure (Tensor cannot be logged as a scalar)\n",
    "        #fig, ax = plt.subplots(figsize=(20,20))\n",
    "        #disp = ConfusionMatrixDisplay(confusion_matrix=self.confusion_matrix.compute().cpu().numpy(),\n",
    "                                      display_labels=self.label_names)\n",
    "        #disp.plot(ax=ax)\n",
    "        # log figure\n",
    "        #self.logger.experiment.add_figure(stage+'/confmat', fig, global_step=self.global_step)\n",
    "        \n",
    "        #np.save(CM_FILE_NAME, self.train_cm.compute().cpu().numpy())\n",
    "    \n",
    "        #self.confusion_matrix.reset()\n",
    "        return       \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")            \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"valid\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer=torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=0.0001, weight_decay=1e-5)\n",
    "        optimizer=torch.optim.AdamW( self.conv_1d.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        #scheduler = CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "    \n",
    "        return {'optimizer': optimizer,'lr_scheduler':scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = SequentialImageVirtualKittiDataset(self.dataset_path, \"train\", transforms=self.kornia_pre_transform)\n",
    "        loader = DataLoader(dataset, batch_size=10, shuffle=False, num_workers=10)\n",
    "        self.label_names = dataset.label_names\n",
    "        print ('training dataset length : ', len(dataset))\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = SequentialImageVirtualKittiDataset(self.dataset_path, \"valid\", transforms=self.kornia_pre_transform)\n",
    "        loader = DataLoader(dataset, batch_size=10, shuffle=False, num_workers=10)\n",
    "        self.label_names = dataset.label_names\n",
    "        print ('Vaidation dataset length : ', len(dataset))\n",
    "        return loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
