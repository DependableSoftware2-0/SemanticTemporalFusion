{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81da3eb7",
   "metadata": {},
   "source": [
    "# Big Problem\n",
    "\n",
    "We dont want the datloader to read and load the depth while training to save time .\n",
    "\n",
    "So if we ant to simulatneously want to train then we have to load depth even for the other images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7730ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import h5py\n",
    "\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from torchmetrics import ConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f43efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader_utils #Not using the above dataset but from the class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad34755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 480\n",
      "Valid size: 54\n",
      " CPU  16\n"
     ]
    }
   ],
   "source": [
    "root = '../../learning_blenerproc/images_robocup'\n",
    "# init train, val, test sets\n",
    "train_dataset = dataloader_utils.SimpleRoboCupDataset(root, \"train\")\n",
    "valid_dataset = dataloader_utils.SimpleRoboCupDataset(root, \"valid\")\n",
    "\n",
    "# It is a good practice to check datasets don`t intersects with each other\n",
    "assert set(train_dataset.filenames).isdisjoint(set(valid_dataset.filenames))\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Valid size: {len(valid_dataset)}\")\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "n_batch_size = 32\n",
    "print (\" CPU \", n_cpu)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=n_batch_size, shuffle=True, num_workers=int(n_cpu/2))\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=n_batch_size, shuffle=False, num_workers=int(n_cpu/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93205e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoboCupModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "        )\n",
    "\n",
    "        # preprocessing parameteres for image\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn_without_background = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, \n",
    "                                                              from_logits=True, \n",
    "                                                              ignore_index=0.0)\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=True)\n",
    "        self.n_classes = out_classes\n",
    "        \n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.confmat = ConfusionMatrix(num_classes=out_classes, normalize='none')\n",
    "        self.fusion_1d = torch.nn.Conv2d(in_channels=2*self.n_classes, out_channels=self.n_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        # normalize image here\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage, optimizer_idx):\n",
    "        \n",
    "        # train model\n",
    "        if optimizer_idx == 0:\n",
    "        image = batch[\"image\"]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32, \n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
    "        # for binary segmentation num_classes = 1\n",
    "        assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 255.0 and mask.min() >= 0\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "        \n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        if (stage == \"train\"):\n",
    "            if self.epoch % 3 == 0:\n",
    "                loss = self.loss_fn(logits_mask, mask.long())\n",
    "            else:\n",
    "                #loss = self.loss_fn(logits_mask, mask.long())\n",
    "                loss = self.loss_fn_without_background(logits_mask, mask.long())\n",
    "        else:\n",
    "            #loss = self.loss_fn(logits_mask, mask.long())\n",
    "            loss = self.loss_fn_without_background(logits_mask, mask.long())\n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then \n",
    "        # apply thresholding\n",
    "        #prob_mask = logits_mask.sigmoid()\n",
    "        #pred_mask = (prob_mask > 0.5).float()\n",
    "        prob_mask = logits_mask.log_softmax(dim=1).exp()\n",
    "        pred_mask = prob_mask.argmax(dim=1, keepdim=True)\n",
    "        #print (\"prob mask \",prob_mask.shape)\n",
    "        #print (\"pred mask \",pred_mask.shape)\n",
    "        #print (\" mask \", mask.shape)\n",
    "        #Confusion matrix calculation\n",
    "        confusion_matrix = self.confmat(pred_mask.long().ravel(), mask.long().ravel())\n",
    "\n",
    "\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"multiclass\", \n",
    "                                               num_classes=self.n_classes)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "            \"confusion_matrix\":confusion_matrix,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image \n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset \n",
    "        # with \"empty\" images (images without target class) a large gap could be observed. \n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        \n",
    "         #confusion matrix sum\n",
    "        self.cm = torch.sum(torch.stack([x[\"confusion_matrix\"] for x in outputs]), dim=0)\n",
    "        \n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        self.epoch += 1\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        return self.shared_step(batch, \"train\", optimizer_idx)            \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, optimizer_idx):\n",
    "        return self.shared_step(batch, \"valid\", optimizer_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"test\")  \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_model=torch.optim.AdamW(self.model.parameters(), lr=0.0001, weight_decay=1e-5, amsgrad=True)\n",
    "        scheduler_model = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-4, last_epoch=-1)\n",
    "        optimizer_fusion1d=torch.optim.AdamW(self.fusion_1d.parameters(), lr=0.0001, weight_decay=1e-5, amsgrad=True)\n",
    "        scheduler_fusion1d = CosineAnnealingWarmRestarts(optimizer_fusion1d, T_0=10, T_mult=2, \n",
    "                                                         eta_min=1e-4, last_epoch=-1)\n",
    "        return {'optimizer': [optimizer_model, optimizer_fusion1d],\n",
    "                'lr_scheduler':[scheduler_model, scheduler_fusion1d]}\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
